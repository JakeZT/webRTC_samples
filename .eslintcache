[{"E:\\test\\webrtcjake\\src\\index.js":"1","E:\\test\\webrtcjake\\src\\App.js":"2","E:\\test\\webrtcjake\\src\\Examples\\media-settings\\MediaSetting.js":"3","E:\\test\\webrtcjake\\src\\Examples\\Camera.jsx":"4","E:\\test\\webrtcjake\\src\\Examples\\Samples.jsx":"5","E:\\test\\webrtcjake\\src\\Examples\\ScreenShare.jsx":"6","E:\\test\\webrtcjake\\src\\Examples\\Microphone.jsx":"7","E:\\test\\webrtcjake\\src\\Examples\\VideoFilter.jsx":"8","E:\\test\\webrtcjake\\src\\Examples\\Resolution.jsx":"9","E:\\test\\webrtcjake\\src\\Examples\\Canvas.jsx":"10","E:\\test\\webrtcjake\\src\\Examples\\MediaStreamAPI.jsx":"11","E:\\test\\webrtcjake\\src\\Examples\\RecordVideo.jsx":"12","E:\\test\\webrtcjake\\src\\Examples\\RecordAudio.jsx":"13","E:\\test\\webrtcjake\\src\\Examples\\DeviceSelect.jsx":"14","E:\\test\\webrtcjake\\src\\Examples\\RecordScreen.jsx":"15","E:\\test\\webrtcjake\\src\\Examples\\CaptureVideo.jsx":"16","E:\\test\\webrtcjake\\src\\Examples\\PeerConnectionVideo.jsx":"17","E:\\test\\webrtcjake\\src\\Examples\\DataChannelFile.jsx":"18","E:\\test\\webrtcjake\\src\\Examples\\PeerConnectionCanvas.jsx":"19","E:\\test\\webrtcjake\\src\\Examples\\PeerConnection.jsx":"20","E:\\test\\webrtcjake\\src\\Examples\\RecordCanvas.jsx":"21","E:\\test\\webrtcjake\\src\\Examples\\DataChannel.jsx":"22","E:\\test\\webrtcjake\\src\\Examples\\volume\\AudioVolume.jsx":"23","E:\\test\\webrtcjake\\src\\Examples\\media-settings\\soundmeter.js":"24","E:\\test\\webrtcjake\\src\\Examples\\volume\\soundmeter.js":"25","E:\\test\\webrtcjake\\src\\router\\index.js":"26","E:\\test\\webrtcjake\\src\\Examples\\Error.jsx":"27","E:\\test\\webrtcjake\\src\\Examples\\CaptureCanvas.jsx":"28"},{"size":261,"mtime":1608935153634,"results":"29","hashOfConfig":"30"},{"size":380,"mtime":1608935248961,"results":"31","hashOfConfig":"30"},{"size":9705,"mtime":1608933144504,"results":"32","hashOfConfig":"30"},{"size":1368,"mtime":1608933112182,"results":"33","hashOfConfig":"30"},{"size":1801,"mtime":1608937425848,"results":"34","hashOfConfig":"30"},{"size":904,"mtime":1608933112107,"results":"35","hashOfConfig":"30"},{"size":1105,"mtime":1608933111900,"results":"36","hashOfConfig":"30"},{"size":1937,"mtime":1608934494248,"results":"37","hashOfConfig":"30"},{"size":4085,"mtime":1608933112059,"results":"38","hashOfConfig":"30"},{"size":1953,"mtime":1608933317420,"results":"39","hashOfConfig":"30"},{"size":2405,"mtime":1608934609978,"results":"40","hashOfConfig":"30"},{"size":4314,"mtime":1608933112037,"results":"41","hashOfConfig":"30"},{"size":3388,"mtime":1608933111973,"results":"42","hashOfConfig":"30"},{"size":4926,"mtime":1608933111867,"results":"43","hashOfConfig":"30"},{"size":2772,"mtime":1608933112015,"results":"44","hashOfConfig":"30"},{"size":1695,"mtime":1608934586852,"results":"45","hashOfConfig":"30"},{"size":7960,"mtime":1608934439896,"results":"46","hashOfConfig":"30"},{"size":8883,"mtime":1608934601977,"results":"47","hashOfConfig":"30"},{"size":8239,"mtime":1608933111934,"results":"48","hashOfConfig":"30"},{"size":7991,"mtime":1608933111917,"results":"49","hashOfConfig":"30"},{"size":3060,"mtime":1608933111992,"results":"50","hashOfConfig":"30"},{"size":7190,"mtime":1608934595526,"results":"51","hashOfConfig":"30"},{"size":1662,"mtime":1608934433651,"results":"52","hashOfConfig":"30"},{"size":1523,"mtime":1608933144505,"results":"53","hashOfConfig":"30"},{"size":1563,"mtime":1608933144467,"results":"54","hashOfConfig":"30"},{"size":4839,"mtime":1608934937100,"results":"55","hashOfConfig":"30"},{"size":159,"mtime":1608936054299,"results":"56","hashOfConfig":"30"},{"size":1755,"mtime":1608934897928,"results":"57","hashOfConfig":"30"},{"filePath":"58","messages":"59","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"60"},"1xyy6w7",{"filePath":"61","messages":"62","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"60"},{"filePath":"63","messages":"64","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"65","usedDeprecatedRules":"60"},{"filePath":"66","messages":"67","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"60"},{"filePath":"68","messages":"69","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},{"filePath":"70","messages":"71","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"60"},{"filePath":"72","messages":"73","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"74","usedDeprecatedRules":"60"},{"filePath":"75","messages":"76","errorCount":0,"warningCount":2,"fixableErrorCount":0,"fixableWarningCount":0,"source":"77","usedDeprecatedRules":"60"},{"filePath":"78","messages":"79","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"80","usedDeprecatedRules":"60"},{"filePath":"81","messages":"82","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"83","usedDeprecatedRules":"60"},{"filePath":"84","messages":"85","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"86","usedDeprecatedRules":"60"},{"filePath":"87","messages":"88","errorCount":0,"warningCount":6,"fixableErrorCount":0,"fixableWarningCount":0,"source":"89","usedDeprecatedRules":"60"},{"filePath":"90","messages":"91","errorCount":0,"warningCount":6,"fixableErrorCount":0,"fixableWarningCount":0,"source":"92","usedDeprecatedRules":"60"},{"filePath":"93","messages":"94","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"95","usedDeprecatedRules":"60"},{"filePath":"96","messages":"97","errorCount":0,"warningCount":3,"fixableErrorCount":0,"fixableWarningCount":0,"source":"98","usedDeprecatedRules":"60"},{"filePath":"99","messages":"100","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"101","usedDeprecatedRules":"60"},{"filePath":"102","messages":"103","errorCount":0,"warningCount":2,"fixableErrorCount":0,"fixableWarningCount":0,"source":"104","usedDeprecatedRules":"60"},{"filePath":"105","messages":"106","errorCount":0,"warningCount":2,"fixableErrorCount":0,"fixableWarningCount":0,"source":"107","usedDeprecatedRules":"60"},{"filePath":"108","messages":"109","errorCount":0,"warningCount":4,"fixableErrorCount":0,"fixableWarningCount":0,"source":"110","usedDeprecatedRules":"60"},{"filePath":"111","messages":"112","errorCount":0,"warningCount":2,"fixableErrorCount":0,"fixableWarningCount":0,"source":"113","usedDeprecatedRules":"60"},{"filePath":"114","messages":"115","errorCount":0,"warningCount":3,"fixableErrorCount":0,"fixableWarningCount":0,"source":"116","usedDeprecatedRules":"60"},{"filePath":"117","messages":"118","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"60"},{"filePath":"119","messages":"120","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"121","usedDeprecatedRules":"60"},{"filePath":"122","messages":"123","errorCount":0,"warningCount":2,"fixableErrorCount":0,"fixableWarningCount":1,"source":"124","usedDeprecatedRules":"60"},{"filePath":"125","messages":"126","errorCount":0,"warningCount":2,"fixableErrorCount":0,"fixableWarningCount":1,"source":"127","usedDeprecatedRules":"128"},{"filePath":"129","messages":"130","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"60"},{"filePath":"131","messages":"132","errorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":null},{"filePath":"133","messages":"134","errorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0,"usedDeprecatedRules":"60"},"E:\\test\\webrtcjake\\src\\index.js",[],["135","136"],"E:\\test\\webrtcjake\\src\\App.js",[],"E:\\test\\webrtcjake\\src\\Examples\\media-settings\\MediaSetting.js",["137"],"import React, { useRef, useEffect, useState } from 'react'\nimport { Button, Select, Modal } from 'antd'\nimport SoundMeter from './soundmeter'\nimport '../../styles/css/media-settings.scss'\nconst { Option } = Select\n\nlet videoElement\nconst MediaSettings = (props) => {\n  const previewVideoRef = useRef()\n  const progressBar = useRef()\n  const timer = useRef()\n  const timer2 = useRef()\n\n\n  const [visible, visibleChange] = useState(false)\n  const [videoDevices, videoDevicesChange] = useState([])\n  const [audioDevices, audioDevicesChange] = useState([])\n  const [audioOutputDevices, audioOutputDevicesChange] = useState([])\n  const [selectedAudioOutputDevice, selectedAudioOutputDeviceChange] = useState('')\n  const [resolution, resolutionChange] = useState('vga')\n  const [selectedAudioDevice, selectedAudioDeviceChange] = useState(\"\")\n  const [selectedVideoDevice, selectedVideoDeviceChange] = useState('')\n  const [audioLevel, audioLevelChange] = useState(0)\n\n  try {\n    window.AudioContext = window.AudioContext || window.webkitAudioContext\n    window.audioContext = new AudioContext()\n  } catch (e) {\n    console.log('网页音频API不支持.')\n  }\n\n  useEffect(() => {\n    if (!!window.localStorage) {\n      let deviceInfo = localStorage['deviceInfo']\n      if (deviceInfo) {\n        let info = JSON.parse(deviceInfo)\n\n        selectedAudioDeviceChange(info.audioDevice)\n        selectedVideoDeviceChange(info.videoDevice)\n        resolutionChange(info.resolution)\n      }\n    }\n\n    updateDevices().then((data) => {\n      if (selectedAudioDevice === '' && data.audioDevices.length > 0) {\n        selectedAudioDeviceChange(data.audioDevices[0].deviceId)\n      }\n      if (\n        selectedAudioOutputDevice === '' &&\n        data.audioOutputDevices.length > 0\n      ) {\n        selectedAudioOutputDeviceChange(data.audioOutputDevices[0].deviceId)\n      }\n      if (selectedVideoDevice === '' && data.videoDevices.length > 0) {\n        selectedVideoDeviceChange(data.videoDevices[0].deviceId)\n      }\n\n      videoDevicesChange(data.videoDevices)\n      audioDevicesChange(data.audioDevices)\n      audioOutputDevicesChange(data.audioOutputDevices)\n    })\n\n    return () => {\n        clearTimeout(timer)\n        clearTimeout(timer2)\n      }\n\n  }, [])\n\n  const updateDevices = () => {\n    return new Promise((resolve, reject) => {\n      let videoDevices = []\n      let audioDevices = []\n      let audioOutputDevices = []\n\n      navigator.mediaDevices\n        .enumerateDevices()\n        .then((devices) => {\n          for (let device of devices) {\n            if (device.kind === 'videoinput') {\n              videoDevices.push(device)\n            } else if (device.kind === 'audioinput') {\n              audioDevices.push(device)\n            }\n            if (device.kind === 'audiooutput') {\n              audioOutputDevices.push(device)\n            }\n          }\n        })\n        .then(() => {\n          let data = { videoDevices, audioDevices, audioOutputDevices }\n          resolve(data)\n        })\n        .catch((error)=>{\n            reject(error)\n        })\n    })\n  }\n\n  const soundMeterProcess = () => {\n    let val = window.soundMeter.instant.toFixed(2) * 348 + 1\n    audioLevelChange(val)\n    if (visible) {\n      timer.current=setTimeout(soundMeterProcess, 100)\n    }\n  }\n\n  const stopPreview = () => {\n    if (!!window.stream) {\n      closeMediaStream(window.stream)\n    }\n  }\n  const handleAudioOutputDeviceChange = (e) => {\n    selectedAudioOutputDeviceChange(e)\n\n    if (typeof videoElement.skinId !== 'undefined') {\n      videoElement\n        .setSinkId(e)\n        .then(() => {\n          console.log('音频输出设备设置成功')\n        })\n        .catch((error) => {\n          // need to use https\n          console.log('音频输出设备设置失败')\n        })\n    } else {\n      console.warn('你的浏览器不支持输出设备选择')\n    }\n  }\n  const startPreview = () => {\n    //\n    if (!!window.stream) {\n      closeMediaStream(window.stream)\n    }\n\n    let soundMeter = window.soundMeter = new SoundMeter(window.audioContext)\n\n    videoElement = previewVideoRef.current\n    let audioSource = selectedAudioDevice\n    let videoSource = selectedVideoDevice\n\n    let constraints = {\n      audio: { deviceId:audioSource? { exact: audioSource }:{exact: audioDevices[0].deviceId} },\n      video: { deviceId: videoSource ? { exact: videoSource } : {exact: videoDevices[0].deviceId} },\n    }\n    console.log(audioDevices[0].deviceId)//default\n    navigator.mediaDevices\n      .getUserMedia(constraints)\n      .then((stream) => {\n        window.stream = stream\n        videoElement.srcObject = stream\n        soundMeter.connectToSource(stream)\n        timer2.current=setTimeout(soundMeterProcess, 100)\n\n        return navigator.mediaDevices.enumerateDevices()\n      })\n      .then((devices) => {})\n      .catch((e) => {\n        console.log(e)\n      })\n  }\n\n  const closeMediaStream = (stream) => {\n    if (!stream) {\n      return\n    }\n\n    let tracks, i, len\n    if (stream.getTracks) {\n      tracks = stream.getTracks()\n      for (i = 0, len = tracks.length; i < len; ++i) {\n        tracks[i].stop()\n      }\n    } else {\n      tracks = stream.getAudioTracks()\n      for (i = 0, len = tracks.length; i < len; ++i) {\n        tracks[i].stop()\n      }\n\n      tracks = stream.getVideoTracks()\n      for (i = 0, len = tracks.length; i < len; ++i) {\n        tracks[i].stop()\n      }\n    }\n  }\n// {\"audioDevice\":\"default\",\"videoDevice\":\"c3b876b39d8abc9e04d38ae203b9936b040db64ae62169d5919293b41c02dc03\",\"resolution\":\"fullhd\"}\n  const showModal = () => {\n    visibleChange(true)\n    setTimeout(startPreview, 100)\n  }\n\n  const handleOk = (e) => {\n    visibleChange(false)\n    // if (!!window.localStorage) {\n      let deviceInfo = {\n        audioDevice: selectedAudioDevice,\n        videoDevice: selectedVideoDevice,\n        resolution: resolution,\n      }\n      localStorage['deviceInfo'] = JSON.stringify(deviceInfo)\n    // }\n    stopPreview()\n  }\n\n  const handleCancel = () => {\n    visibleChange(false)\n    stopPreview()\n  }\n\n  const handleAudioDeviceChange = (e) => {\n    selectedAudioDeviceChange(e)\n    setTimeout(startPreview, 100)\n  }\n\n  const handleVideoDeviceChange = (e) => {\n    selectedVideoDeviceChange(e)\n    setTimeout(startPreview, 100)\n  }\n\n  const handleResolutionChange = (e) => {\n    resolutionChange(e)\n  }\n\n  return (\n    <div className=\"container\">\n      <h1>\n        <span>设置综合示例</span>\n      </h1>\n      <Button onClick={showModal}>修改设备</Button>\n      <Modal\n        title=\"修改设备\"\n        visible={visible}\n        onOk={handleOk}\n        onCancel={handleCancel}\n        okText=\"OK\"\n        cancelText=\"Cancel\"\n      >\n        <div className=\"item\">\n          <span className=\"item-left\">麦克风</span>\n          <div className=\"item-right\">\n            <Select\n              value={selectedAudioDevice}\n              style={{ width: 350 }}\n              onChange={handleAudioDeviceChange}\n            >\n              {audioDevices.map((device, index) => {\n                return (\n                  <Option value={device.deviceId} key={device.deviceId}>\n                    {device.label}\n                  </Option>\n                )\n              })}\n            </Select>\n            <div\n              ref={progressBar}\n              style={{\n                width: audioLevel + 'px',\n                height: '10px',\n                backgroundColor: '#8dc63f',\n                marginTop: '20px',\n              }}\n            ></div>\n          </div>\n        </div>\n        <div className=\"item\">\n          <span className=\"item-left\">摄像头</span>\n          <div className=\"item-right\">\n            <Select\n              value={selectedVideoDevice}\n              style={{ width: 350 }}\n              onChange={handleVideoDeviceChange}\n            >\n              {videoDevices.map((device, index) => {\n                return (\n                  <Option value={device.deviceId} key={device.deviceId}>\n                    {device.label}\n                  </Option>\n                )\n              })}\n            </Select>\n            <div className=\"video-container\">\n              <video\n                id=\"previewVideo\"\n                ref={previewVideoRef}\n                autoPlay\n                playsInline\n                style={{ width: '100%', height: '100%', objectFit: 'contain' }}\n              ></video>\n            </div>\n          </div>\n        </div>\n        <div className=\"item\">\n          <span className=\"item-left\">音频设备</span>\n          <div className=\"item-right\">\n            <Select\n              value={selectedAudioOutputDevice}\n              style={{ width: 350 }}\n              onChange={handleAudioOutputDeviceChange}\n            >\n              {audioOutputDevices.map((device, index) => {\n                return (\n                  <Option value={device.deviceId} key={device.deviceId}>\n                    {device.label}\n                  </Option>\n                )\n              })}\n            </Select>\n          </div>\n        </div>\n        <div className=\"item\">\n          <span className=\"item-left\">清晰度</span>\n          <div className=\"item-right\">\n            <Select\n              style={{ width: 350 }}\n              value={resolution}\n              onChange={handleResolutionChange}\n            >\n              <Option value=\"qvga\">流畅(320x240)</Option>\n              <Option value=\"vga\">标清(640x360)</Option>\n              <Option value=\"hd\">高清(1280x720)</Option>\n              <Option value=\"fullhd\">超清(1920x1080)</Option>\n            </Select>\n          </div>\n        </div>\n      </Modal>\n    </div>\n  )\n}\n\nexport default MediaSettings\n","E:\\test\\webrtcjake\\src\\Examples\\Camera.jsx",[],"E:\\test\\webrtcjake\\src\\Examples\\Samples.jsx",[],"E:\\test\\webrtcjake\\src\\Examples\\ScreenShare.jsx",[],"E:\\test\\webrtcjake\\src\\Examples\\Microphone.jsx",["138"],"import React,{useRef,useEffect} from \"react\";\n\n\nconst constraints = window.constraints ={\n    audio:true,\n    video:false\n}\n\nfunction Microphone(){\nconst audioRef=useRef()\n    \n    useEffect(()=>{\n        openCamera();\n    },[])\n\n    const openCamera = async (e) =>{\n\n        try{\n            const stream = await navigator.mediaDevices.getUserMedia(constraints);\n            console.log('success');\n            handleSuccess(stream);\n\n        }catch(e){\n            handleError(e);\n        }\n\n    }\n\n    const handleSuccess = (stream) =>{\n        const audioTracks = stream.getAudioTracks();\n        console.log('使用的设备是:' + audioTracks[0].label);\n        window.stream = stream;\n        audioRef.current.srcObject = stream;\n    }\n\n    const handleError=(error)=>{\n        console.log('getUserMedia error:' ,error.message,error.name);\n    }\n    return(\n            <div className=\"container\">\n            <h1>\n              <span>麦克风示例</span>\n            </h1>\n            <audio ref={audioRef} controls autoPlay ></audio>\n          </div>   \n        );\n}\n\nexport default Microphone;","E:\\test\\webrtcjake\\src\\Examples\\VideoFilter.jsx",["139","140"],"import React,{useRef,useEffect} from \"react\";\nimport { Button, message, Select } from \"antd\";\nimport '../styles/css/video-filter.scss';\n\nconst { Option } = Select;\nconst constraints = (window.constraints = {\n    video: true,\n    audio: false,\n  })\nconst VideoFilter=()=> {\n    let video=null;\n    const videoRef=useRef();\n    \n    useEffect(()=>{\n        video = videoRef.current;\n        const constraints = {\n            audio: false,\n            video: true\n        }\n\n        navigator.mediaDevices.getUserMedia(constraints).then(handleSuccess).catch((err)=>{handleError(err)});\n    },[])\n\n\n\n    const handleSuccess = (stream) => {\n        window.stream = stream;\n        video.srcObject = stream;\n    }\n\n    const handleError=(error)=>{\n        if (error.name === 'ConstraintNotSatisfiedError') {\n            const v = constraints.video;\n            message.error(`宽:${v.width.exact} 高:${v.height.exact} 设备不支持`);\n        } else if (error.name === 'PermissionDeniedError') {\n            message.error('没有摄像头和麦克风的使用权限,请点击允许按钮');\n        }\n        message.error('getUserMedia错误:', error);\n    }\n\n    const handChange = (value) => {\n        video.className = value;\n    }\n\n\n        return (\n            <div className=\"container\">\n                <h1>\n                    <span>视频滤镜示例</span>\n                </h1>\n                <video ref={videoRef} autoPlay playsInline></video>\n                <Select defaultValue=\"none\" style={{ width: '100px' }} onChange={handChange}>\n                    <Option value=\"none\">没有滤镜</Option>\n                    <Option value=\"blur\">模糊</Option>\n                    <Option value=\"grayscale\">灰度</Option>\n                    <Option value=\"invert\">反转</Option>\n                    <Option value=\"sepia\">深褐色</Option>\n                </Select>\n            </div>\n        );\n}\n\nexport default VideoFilter;","E:\\test\\webrtcjake\\src\\Examples\\Resolution.jsx",["141"],"import React,{useRef,useEffect} from \"react\";\nimport { Button, message,Select } from \"antd\";\nconst { Option } = Select;\n\n/* \nThe reason for the difference in behavior is that the keywords min, max, and exact are inherently mandatory.\n Whereas plain values and a keyword called ideal are not. Here's a full example:\n\n{\n  audio: true,\n  video: {\n    width: { min: 1024, ideal: 1280, max: 1920 },\n    height: { min: 576, ideal: 720, max: 1080 }\n  }\n}\n\n*/\nconst q_vgaConstraints = {\n    video: { width: { exact: 320 }, height: { exact: 240 } },\n    audio:false\n};\n\nconst vgaConstraints = {\n    video: { width: { exact: 640 }, height: { exact: 480 } },\n    audio:false\n};\n\nconst hdConstraints = {\n    video: { width: { min: 1280 }, height: { min: 720 } },\n    audio:false\n};\n\nconst fullHdConstraints = {\n    video: { width: { min: 1920 }, height: { min: 1080 } },\n    audio:false\n};\n\nconst twoKConstraints = {\n    video: { width: { min: 2560 }, height: { min: 1440 } },\n    audio:false\n};\n\nconst fourKConstraints = {\n    video: { width: { min: 4096 }, height: { min: 2160 } },\n    audio:false\n};\n\nconst eightKConstraints = {\n    video: { width: { min: 7680 }, height: { min: 4320 } },\n    audio:false\n};\n\nlet stream;\nlet video;\n\nconst Resolution=()=>{\nconst videoRef=useRef();\nuseEffect(()=>{\n    video=videoRef.current;\n    getMedia(vgaConstraints)\n},[])\n\n    const getMedia = (constraints=0) => {\n        if(constraints===0) return;\n        if (stream) {\n            stream.getTracks().forEach(track => {\n                track.stop();\n            });\n        }\n        navigator.mediaDevices.getUserMedia(constraints).then(gotStream).catch(e => {\n            handleError(e);\n        });\n\n    }\n\n    const gotStream = (mediaStream) => {\n        stream = window.stream = mediaStream;\n        video.srcObject = stream;\n    }\n\n    const handleError=(error) =>{\n        console.log('getUserMedia错误:', error);\n        message.error('Your camera doesn\\'t support this constraint.')\n    }\n\n    const handChange = (value) => {\n        switch (value) {\n            case 'qvga':\n                getMedia(q_vgaConstraints);\n                break;\n            case 'vga':\n                getMedia(vgaConstraints);\n                break;\n            case 'hd':\n                getMedia(hdConstraints);\n                break;\n            case 'fullhd':\n                getMedia(fullHdConstraints);\n                break;\n            case '2k':\n                getMedia(twoKConstraints);\n                break;\n            case '4k':\n                getMedia(fourKConstraints);\n                break;\n            case '8k':\n                getMedia(eightKConstraints);\n                break;\n            default:\n                getMedia(vgaConstraints);\n                break;\n        }\n    }\n\n    const dynamicChange = (e) => {\n        const track = window.stream.getVideoTracks()[0];\n        console.log(window.stream.getVideoTracks());\n        let constraints = vgaConstraints;\n        track.applyConstraints(constraints)\n        .then(() => {\n            console.log(\"动态改变分辨率成功...\");\n        }).catch(err => {\n            console.log(\"动态改变分辨率错误...\",err.name);\n        })\n    }\n\n\n\n        return (\n            <div className=\"container\">\n                <h1>\n                    <span>摄像头示例</span>\n                </h1>\n                <video className=\"video\" ref={videoRef} autoPlay playsInline></video>\n                <Select defaultValue=\"vga\" style={{ width: '100px', marginLeft: '20px' }} onChange={handChange}>\n                    <Option value=\"qvga\">QVGA</Option>\n                    <Option value=\"vga\">VGA</Option>\n                    <Option value=\"hd\">高清</Option>\n                    <Option value=\"fullhd\">超清</Option>\n                    <Option value=\"2k\">2K</Option>\n                    <Option value=\"4k\">4K</Option>\n                    <Option value=\"8k\">8K</Option>\n                </Select>\n                <Button onClick={dynamicChange}>动态设置</Button>\n            </div>\n        );\n\n}\n\nexport default Resolution;","E:\\test\\webrtcjake\\src\\Examples\\Canvas.jsx",["142"],"import React,{useRef,useEffect} from \"react\";\nimport {Button,message} from \"antd\";\nimport '../styles/css/canvas.scss';\n\n\nconst constraints = window.constraints ={\n    audio:false,\n    video:true\n}\n\n\nfunction Canvas(){\n    const videoRef=useRef();\n    const canvasRef=useRef();\n    useEffect(()=>{\n        openCamera() \n    },[])\n\n\n    const openCamera = async (e) =>{\n\n        try{\n            const stream = await navigator.mediaDevices.getUserMedia(constraints);\n            console.log('success');\n            handleSuccess(stream);\n\n        }catch(e){\n            handleError(e);\n        }\n\n    }\n\n    const handleSuccess = (stream) =>{\n        \n        window.stream = stream;\n        videoRef.current.srcObject = stream;\n    }\n\n    const handleError=(error)=>{\n        if(error.name === 'ConstraintNotSatisfiedError'){\n            const v = constraints.video;\n            message.error(`宽:${v.width.exact} 高:${v.height.exact} 设备不支持`);\n        } else if(error.name === 'PermissionDeniedError'){\n            message.error('没有摄像头和麦克风的使用权限,请点击允许按钮');\n        }\n        message.error('getUserMedia错误:',error);\n    }\n\n    const takeSnap = async (e) => {\n        let canvas = canvasRef.current\n        console.log(videoRef.current.videoWidth)\n        console.log(canvasRef)\n        canvas.width = videoRef.current.videoWidth;\n        canvas.height = videoRef.current.videoHeight;\n        canvas.getContext('2d').drawImage(videoRef.current,0,0,canvas.width,canvas.height);\n    }\n\n\n\n        return(\n            <div className=\"container\">\n            <h1>\n              <span>截取视频示例</span>\n            </h1>\n            <video className=\"small-video\" ref={videoRef} autoPlay playsInline></video>\n            <canvas className=\"small-canvas\" ref={canvasRef}></canvas>\n            <Button onClick={takeSnap}>截屏</Button>\n          </div>   \n        );\n    \n\n}\n\nexport default Canvas;","E:\\test\\webrtcjake\\src\\Examples\\MediaStreamAPI.jsx",["143"],"import React,{useEffect,useRef} from \"react\";\nimport {Button,message} from \"antd\";\n\nlet stream;\n\nconst constraints = (window.constraints = {\n    video: true,\n    audio: true,\n  })\nconst MediaStreamAPI=()=> {\n    const myVideo=useRef();\nuseEffect(()=>{\n    openDevice()\n},[])\n\n    const openDevice = async (e) =>{\n        try{\n            stream = await navigator.mediaDevices.getUserMedia({\n                audio:true,\n                video:true\n            });\n            const video =myVideo.current;\n            video.srcObject = stream;\n\n        }catch(e){\n            handleError(e);\n        }\n\n    }\n\n    const handleError=(error)=>{\n        if(error.name === 'ConstraintNotSatisfiedError'){\n            const v = constraints.video;\n            message.error(`宽:${v.width.exact} 高:${v.height.exact} 设备不支持`);\n        } else if(error.name === 'PermissionDeniedError'){\n            message.error('没有摄像头和麦克风的使用权限,请点击允许按钮');\n        }\n        message.error('getUserMedia错误:',error);\n    }\n\n\n    const btnGetTracks = () => {\n        console.log(\"btnGetTracks()\");\n        console.log(stream.getTracks());\n    }\n\n    const btnGetAudioTracks = () => {\n        console.log(\"btnGetAudioTracks()\");\n        console.log(stream.getAudioTracks());\n    }\n\n    const btnGetVideoTracks = () => {\n        console.log(\"btnGetVideoTracks()\");\n        console.log(stream.getVideoTracks());\n    }\n\n    const btnRemoveAudioTracks = () => {\n        console.log(\"btnGetVideoTracks()\");\n        stream.removeTrack(stream.getAudioTracks()[0]);\n    }\n\n    const btnGetTrackById = () => {\n        console.log(\"btnGetTrackById()\");\n        console.log(stream.getTrackById(stream.getAudioTracks()[0].id));\n    }\n\n\n        return(\n            <div className=\"container\">\n            <h1>\n              <span>示例</span>\n            </h1>\n            <video className=\"video\" ref={myVideo} autoPlay playsInline></video>\n            <Button onClick={btnGetTracks}>获取所有轨道</Button>\n            <Button onClick={btnGetAudioTracks}>获取音频轨道</Button>\n            <Button onClick={btnGetVideoTracks}>获取视频轨道</Button>\n            <Button onClick={btnRemoveAudioTracks}>删除音频轨道</Button>\n            <Button onClick={btnGetTrackById}>根据Id获取音频轨道</Button>\n          </div>   \n        );\n\n}\n\nexport default MediaStreamAPI;","E:\\test\\webrtcjake\\src\\Examples\\RecordVideo.jsx",["144","145","146","147","148","149"],"import React, { useState, useEffect, useRef } from 'react'\nimport { Button, message } from 'antd'\nimport '../styles/css/record-video.scss'\n\nlet mediaRecorder\nlet recordedBlobs\nlet videoPreview\nlet videoPlayer\n\nconst RecordVideo = (props) => {\n  const [status, statusChange] = useState('start')\n  const videoPreviewRef = useRef()\n  const videoPlayerRef = useRef()\n\n  useEffect(() => {\n    videoPreview = videoPreviewRef.current\n    videoPlayer = videoPlayerRef.current\n  }, [])\n\n  const startClickHandler = async (e) => {\n    let constraints = {\n      audio: true,\n      video: {\n        width: 1280,\n        height: 720,\n      },\n    }\n\n    try {\n      const stream = await navigator.mediaDevices.getUserMedia(constraints)\n\n      window.stream = stream\n      videoPreview.srcObject = stream\n\n      statusChange('startRecord')\n    } catch (e) {\n      console.log('navigator.mediaDevices.getUserMedia:', e)\n    }\n  }\n\n  const startRecordButtonClickHandler = (e) => {\n    recordedBlobs = []\n\n    let options = { mineType: 'video/webm;codecs=vp9' }\n\n    if (!MediaRecorder.isTypeSupported(options.mineType)) {\n      console.log('video/webm;codecs=vp9不支持')\n      options = { mineType: 'video/webm;codecs=vp8' }\n      if (!MediaRecorder.isTypeSupported(options.mineType)) {\n        console.log('video/webm;codecs=vp8不支持')\n        options = { mineType: 'video/webm' }\n        if (!MediaRecorder.isTypeSupported(options.mineType)) {\n          console.log('video/webm不支持')\n          options = { mineType: '' }\n        }\n      }\n    }\n\n    try {\n      mediaRecorder = new MediaRecorder(window.stream, options)\n    } catch (e) {\n      console.log('MediaRecorder:', e)\n      return\n    }\n\n    mediaRecorder.onstop = (event) => {\n      console.log('Recorder stopped:', event)\n      console.log('Recorder blobs:', recordedBlobs)\n    }\n\n    mediaRecorder.ondataavailable = handleDataAvailable\n\n    mediaRecorder.start(10)\n\n    statusChange('stopRecord')\n  }\n\n  const handleDataAvailable = (event) => {\n    if (event.data && event.data.size > 0) {\n      recordedBlobs.push(event.data)\n    }\n  }\n  \n\n  const stopRecordButtonClickHandler = (e) => {\n    mediaRecorder.stop()\n    statusChange('play')\n  }\n\n  const playButtonClickHandler = (e) => {\n    const blob = new Blob(recordedBlobs, { type: 'video/webm' })\n\n    videoPlayer.src = null\n    videoPlayer.src = window.URL.createObjectURL(blob)\n    videoPlayer.controls = true\n    videoPlayer.play()\n    statusChange('download')\n  }\n\n  const downloadButtonClickHandler = (e) => {\n    const blob = new Blob(recordedBlobs, { type: 'video/webm' })\n    const url = window.URL.createObjectURL(blob)\n\n    const link = document.createElement('a')\n    link.style.display = 'none'\n    link.href = url\n    link.download = 'videoRecord.webm'\n    document.body.appendChild(link)\n    link.click()\n    setTimeout(() => {\n      document.body.removeChild(link)\n      window.URL.revokeObjectURL(url)\n    }, 100)\n    statusChange('start')\n  }\n\n\n\n  return (\n    <div className=\"container\">\n      <h1>\n        <span>示例</span>\n      </h1>\n\n      <video\n        className=\"small-video\"\n        ref={videoPreviewRef}\n        playsInline\n        autoPlay\n        muted\n      ></video>\n      <video\n        className=\"small-video\"\n        ref={videoPlayerRef}\n        playsInline\n        loop\n      ></video>\n\n      <div>\n        <Button\n          className=\"button\"\n          onClick={startClickHandler}\n          disabled={status != 'start'}\n        >\n          打开摄像头\n        </Button>\n        <Button\n          className=\"button\"\n          onClick={startRecordButtonClickHandler}\n          disabled={status != 'startRecord'}\n        >\n          开始录制\n        </Button>\n        <Button\n          className=\"button\"\n          onClick={stopRecordButtonClickHandler}\n          disabled={status != 'stopRecord'}\n        >\n          停止录制\n        </Button>\n        <Button\n          className=\"button\"\n          onClick={playButtonClickHandler}\n          disabled={status != 'play'}\n        >\n          播放\n        </Button>\n        <Button\n          className=\"button\"\n          onClick={downloadButtonClickHandler}\n          disabled={status != 'download'}\n        >\n          下载\n        </Button>\n      </div>\n    </div>\n  )\n}\n\nexport default RecordVideo\n","E:\\test\\webrtcjake\\src\\Examples\\RecordAudio.jsx",["150","151","152","153","154","155"],"import React, { useEffect, useState, useRef } from 'react'\nimport { Button, message } from 'antd'\nimport '../styles/css/record-audio.scss'\n\nlet mediaRecorder\nlet recordedBlobs\nlet audioPlayer\n\nconst RecordAudio = (props) => {\n  const audioPlayerRef = useRef()\n  const [status, statusChange] = useState('start')\n\n  useEffect(() => {\n    audioPlayer = audioPlayerRef.current\n  }, [])\n\n  const startClickHandler = async (e) => {\n    try {\n      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n\n      window.stream = stream\n\n      statusChange('startRecord')\n    } catch (e) {\n      console.log('navigator.mediaDevices.getUserMedia:', e)\n    }\n  }\n\n  const startRecordButtonClickHandler = (e) => {\n    recordedBlobs = []\n\n    let options = { mineType: 'audio/ogg;' }\n\n    try {\n      mediaRecorder = new MediaRecorder(window.stream, options)\n    } catch (e) {\n      console.log('MediaRecorder:', e)\n      return\n    }\n\n    mediaRecorder.onstop = (event) => {\n      console.log('Recorder stopped:', event)\n      console.log('Recorder blobs:', recordedBlobs)\n    }\n\n    mediaRecorder.ondataavailable = handleDataAvailable //不断push进音频流\n\n    mediaRecorder.start(10)\n\n    statusChange('stopRecord')\n  }\n\n  const stopRecordButtonClickHandler = (e) => {\n    mediaRecorder.stop()\n    statusChange('play')\n  }\n\n  const playButtonClickHandler = (e) => {\n    const blob = new Blob(recordedBlobs, { type: 'audio/ogg' })\n\n    audioPlayer.src = null\n    audioPlayer.src = window.URL.createObjectURL(blob)\n    audioPlayer.play()\n    statusChange('download')\n  }\n\n  const downloadButtonClickHandler = (e) => {\n    //   新建一个a标签，然后点击后删除掉\n    const blob = new Blob(recordedBlobs, { type: 'audio/ogg' })\n    const url = window.URL.createObjectURL(blob)\n\n    const link = document.createElement('a')\n    link.style.display = 'none'\n    link.href = url\n    link.download = 'test.ogg'\n    document.body.appendChild(link)\n    link.click()\n    setTimeout(() => {\n      document.body.removeChild(link)\n      window.URL.revokeObjectURL(url)\n    }, 100)\n    statusChange('start')\n  }\n\n  const handleDataAvailable = (event) => {\n    if (event.data && event.data.size > 0) {\n      recordedBlobs.push(event.data)\n    }\n  }\n\n  return (\n    <div className=\"container\">\n      <h1>\n        <span>示例</span>\n      </h1>\n      <audio ref={audioPlayerRef} autoPlay controls></audio>\n      <div>\n        <Button\n          className=\"button\"\n          onClick={startClickHandler}\n          disabled={status != 'start'}\n        >\n          打开麦克风\n        </Button>\n        <Button\n          className=\"button\"\n          onClick={startRecordButtonClickHandler}\n          disabled={status != 'startRecord'}\n        >\n          开始录制\n        </Button>\n        <Button\n          className=\"button\"\n          onClick={stopRecordButtonClickHandler}\n          disabled={status != 'stopRecord'}\n        >\n          停止录制\n        </Button>\n        <Button\n          className=\"button\"\n          onClick={playButtonClickHandler}\n          disabled={status != 'play'}\n        >\n          播放\n        </Button>\n        <Button\n          className=\"button\"\n          onClick={downloadButtonClickHandler}\n          disabled={status != 'download'}\n        >\n          下载\n        </Button>\n      </div>\n    </div>\n  )\n}\n\nexport default RecordAudio\n","E:\\test\\webrtcjake\\src\\Examples\\DeviceSelect.jsx",["156"],"import React, { useEffect, useState, useRef } from 'react'\nimport { Button, Select } from 'antd'\n\nconst { Option } = Select\n\nlet videoElement\n\nconst DeviceSelect = () => {\n  const previewVideo = useRef()\n  \n  const [videoDevices, videoDevicesChange] = useState([]) \n  const [audioDevices, audioDevicesChange] = useState([])\n  const [audioOutputDevices, audioOutputDevicesChange] = useState([])\n  const [selectedAudioDevice, selectedAudioDeviceChange] = useState('')\n  const [selectedAudioOutputDevice, selectedAudioOutputDeviceChange] = useState('')\n  const [selectedVideoDevice, selectedVideoDeviceChange] = useState('')\n\n  useEffect(() => {\n    videoElement = previewVideo.current\n    updateDevices().then((data) => {\n      if (selectedAudioDevice === '' && data.audioDevices.length > 0) {\n        selectedAudioDeviceChange(data.audioDevices[0].deviceId)\n      }\n\n      if (\n        selectedAudioOutputDevice === '' &&\n        data.audioOutputDevices.length > 0\n      ) {\n        selectedAudioOutputDeviceChange(data.audioOutputDevices[0].deviceId)\n      }\n\n      if (selectedVideoDevice === '' && data.videoDevices.length > 0) {\n        selectedVideoDeviceChange(data.videoDevices[0].deviceId)\n      }\n      videoDevicesChange(data.videoDevices)\n      audioDevicesChange(data.audioDevices)\n      audioOutputDevicesChange(data.audioOutputDevices)\n    })\n  }, [])\n\n\n  const updateDevices = () => {\n    return new Promise((resolve, reject) => {\n      let videoDevices = []\n      let audioDevices = []\n      let audioOutputDevices = []\n\n      navigator.mediaDevices\n        .enumerateDevices()\n        .then((devices) => {\n            // return the lists\n          for (let device of devices) {\n            if (device.kind === 'videoinput') {\n              videoDevices.push(device)\n            } else if (device.kind === 'audioinput') {\n              audioDevices.push(device)\n            }\n            if (device.kind === 'audiooutput') {\n              audioOutputDevices.push(device)\n            }\n          }\n        })\n        .then(() => {\n          let data = { videoDevices, audioDevices, audioOutputDevices }\n          resolve(data)\n        })\n    })\n  }\n\n  const handleAudioDeviceChange = (e) => {\n    selectedAudioDeviceChange(e)\n    setTimeout(startTest, 100)// delay to start\n  }\n\n  const handleVideoDeviceChange = (e) => {\n    selectedVideoDeviceChange(e)\n    setTimeout(startTest, 100)\n  }\n\n  const handleAudioOutputDeviceChange = (e) => {\n    selectedAudioOutputDeviceChange(e)\n\n    if (typeof videoElement.skinId !== 'undefined') {\n      videoElement\n        .setSinkId(e)\n        .then(() => {\n          console.log('音频输出设备设置成功')\n        })\n        .catch((error) => {\n            // need to use https\n          console.log('音频输出设备设置失败')\n        })\n    } else {\n      console.warn('你的浏览器不支持输出设备选择')\n    }\n  }\n\n  const startTest = () => {\n    let audioSource = selectedAudioDevice\n    let videoSource = selectedVideoDevice\n\n    let constraints = {\n      audio: { deviceId: audioSource ? { exact: audioSource } : undefined },\n      video: { deviceId: videoSource ? { exact: videoSource } : undefined },\n    }\n    navigator.mediaDevices\n      .getUserMedia(constraints)\n      .then((stream) => {\n        window.stream = stream\n        videoElement.srcObject = stream\n      })\n      .catch((e) => {\n        console.log(e)\n      })\n  }\n\n  return (\n    <div className=\"container\">\n      <h1>\n        <span>设备枚举示例</span>\n      </h1>\n      <Select\n        value={selectedAudioDevice}\n        style={{ width: 150, marginRight: '10px' }}\n        onChange={handleAudioDeviceChange}\n      >\n        {audioDevices.map((device, index) => {\n          return (\n            <Option value={device.deviceId} key={device.deviceId}>\n              {device.label}\n            </Option>\n          )\n        })}\n      </Select>\n      <Select\n        value={selectedAudioOutputDevice}\n        style={{ width: 150, marginRight: '10px' }}\n        onChange={handleAudioOutputDeviceChange}\n      >\n        {audioOutputDevices.map((device, index) => {\n          return (\n            <Option value={device.deviceId} key={device.deviceId}>\n              {device.label}\n            </Option>\n          )\n        })}\n      </Select>\n      <Select\n        value={selectedVideoDevice}\n        style={{ width: 150 }}\n        onChange={handleVideoDeviceChange}\n      >\n        {videoDevices.map((device, index) => {\n          return (\n            <Option value={device.deviceId} key={device.deviceId}>\n              {device.label}\n            </Option>\n          )\n        })}\n      </Select>\n      <video\n        className=\"video\"\n        ref={previewVideo}\n        autoPlay\n        playsInline\n        style={{ objectFit: 'contain', marginTop: '10px' }}\n      ></video>\n      <Button onClick={startTest}>测试</Button>\n    </div>\n  )\n}\n\nexport default DeviceSelect\n","E:\\test\\webrtcjake\\src\\Examples\\RecordScreen.jsx",["157","158","159"],"import React,{useRef,useEffect,useState} from \"react\";\nimport { Button, message } from \"antd\";\n\nlet mediaRecorder;\n\nlet recordedBlobs;\n\nlet stream;\n\n\n\nconst RecordScreen =()=> {\n    const myVideoRef=useRef();\n    const startCaptureScreen = async (e) => {\n\n        try {\n            stream = await navigator.mediaDevices.getDisplayMedia({\n                video: {\n                    width: 2880, height: 1800\n                },\n                audio:true\n            }\n\n            );\n\n            const video =myVideoRef.current\n\n            window.stream = stream;\n            video.srcObject = stream;\n\n            startRecord();\n\n        } catch (e) {\n            console.log(e);\n        }\n    }\n\n    const startRecord = (e) => {\n        // 不活动状态，停止共享屏幕那个停止按钮\n        stream.addEventListener('inactive',e =>{\n            stopRecord(e);\n        });\n\n        recordedBlobs = [];\n\n        try {\n            mediaRecorder = new MediaRecorder(window.stream, { mineType: 'video/webm' });\n        } catch (e) {\n            console.log(\"MediaRecorder:\", e);\n            return;\n        }\n\n        mediaRecorder.onstop = (event) => {\n            console.log('Recorder stopped:', event);\n            console.log('Recorder blobs:', recordedBlobs);\n        }\n\n        mediaRecorder.ondataavailable = handleDataAvailable;\n        mediaRecorder.start(10);\n\n    }\n    const handleDataAvailable = (event) => {\n\n        if (event.data && event.data.size > 0) {\n            recordedBlobs.push(event.data);\n        }\n    }\n\n    const stopRecord = (e) => {\n        mediaRecorder.stop();\n        \n        const blob = new Blob(recordedBlobs, { type: 'video/webm' });\n        const url = window.URL.createObjectURL(blob);\n\n        const link = document.createElement('a');\n        link.style.display = 'none';\n        link.href = url;\n        link.download = 'screen.webm';\n        document.body.appendChild(link);\n        link.click();//调用下载\n        setTimeout(() => {\n            document.body.removeChild(link);\n            window.URL.revokeObjectURL(url);\n        }, 100);\n       \n    }\n\n\n  \n        return (\n            <div className=\"container\">\n                <h1>\n                    <span>示例</span>\n                </h1>\n\n                <video className=\"video\" ref={myVideoRef} playsInline autoPlay></video>\n\n                <Button\n                    className=\"button\"\n                    onClick={startCaptureScreen}\n                    >\n                    开始\n            </Button>\n                <Button\n                    className=\"button\"\n                    onClick={stopRecord}\n                    >\n                    停止\n            </Button>\n            </div>\n        );\n\n}\n\nexport default RecordScreen;","E:\\test\\webrtcjake\\src\\Examples\\CaptureVideo.jsx",["160"],"import React,{useRef} from \"react\";\n\n//? 问题 如果单独获取音频，并使用\nconst constraints = window.constraints ={\n    audio:false,\n    video:true\n}\n//  \nconst CaptureVideo=()=>{\n    const sourceVideo=useRef();\n    const playerVideo=useRef();\n\n\n    const canPlay = () => {\n        const sourceVideoR = sourceVideo.current\n        const playerVideoR = playerVideo.current\n        // console.log(sourceVideo)\n        let stream;\n/* 也可以多个数据源连接到这个流，并且还可以与该new MediaStream([stream1, stream2])构造另一个视频流结合起来 */\n        //frame per second\n        const fps = 0;\n\n        stream = sourceVideoR.captureStream(fps); //如果是firefox,则需要用mozCaptureStream捕获，需要兼容\n        // console.log(stream.getAudioTracks())\n        // let audioStream=stream.getAudioTracks()[0]\n        // let ctx = new AudioContext(); \n        // var source = ctx.createMediaElementSource(sourceVideoR); \n        // let stream_dest = ctx.createMediaStreamDestination();\n        // source.connect(stream_dest); \n        // audioStream.connectToSource(stream)\n        // playerVideoR.srcObject = stream_dest.stream;\n        playerVideoR.srcObject = stream;\n    }\n\n        return(\n            <div className=\"container\">\n            <h1>\n              <span>Example</span>\n            </h1>\n            <video className=\"video\" ref={sourceVideo} controls loop muted playsInline onCanPlay={canPlay}>\n                <source src=\"./assets/webrtc.mp4\" type=\"video/mp4\"/>\n            </video>\n            <video className=\"video\" ref={playerVideo}  playsInline autoPlay></video>\n          </div>   \n        );\n}\n\nexport default CaptureVideo;","E:\\test\\webrtcjake\\src\\Examples\\PeerConnectionVideo.jsx",["161","162"],"import React,{useState,useRef,useEffect} from \"react\";\nimport {Button,message} from \"antd\";\n\n\nlet localVideo;\nlet remoteVideo;\nlet localStream;\nlet peerConnA;\nlet peerConnB;\n\nconst PeerConnectionVideo=()=>{\n    const localVideoRef=useRef();\n    const remoteVideoRef=useRef();\n\n    const canPlay = () => {    \n        //帧数\n        const fps = 0;\n        // 抓取视频流\n        localStream = localVideo.captureStream(fps); \n    }\n\n    useEffect(()=>{\n        localVideo = localVideoRef.current\n        remoteVideo = remoteVideoRef.current\n        //视频清晰度自适应 \n        localVideo.addEventListener('loadedmetadata',() => {\n            console.log(\"本地视频尺寸为:\" + localVideo.videoWidth + \" \" + localVideo.videoHeight);\n        });\n\n        remoteVideo.addEventListener('loadedmetadata',() => {\n            console.log(\"远端视频尺寸为:\" + remoteVideo.videoWidth + \" \" + remoteVideo.videoHeight);\n        });\n\n        remoteVideo.addEventListener('resize',() => {\n            console.log(\"远端视频尺寸为:\" + remoteVideo.videoWidth + \" \" + remoteVideo.videoHeight);\n        });\n    },[])\n\n\n    \n    const call = async () => {\n\n        //视频轨道\n        const videoTracks = localStream.getVideoTracks();\n        //音频轨道\n        const audioTracks = localStream.getAudioTracks();\n        //判断视频轨道是否有值\n        if (videoTracks.length > 0) {\n        //输出摄像头的名称\n        console.log(`使用的视频设备为: ${videoTracks[0].label}`);\n        }\n        //判断音频轨道是否有值\n        if (audioTracks.length > 0) {\n        //输出麦克风的名称\n        console.log(`使用的音频设备为: ${audioTracks[0].label}`);\n        }\n        // 设置iceServer, 使用google服务器\n        let configuration = {\"iceServers\":[{ \"url\": \"stun:stun.l.google.com:19302\" }]};\n\n        peerConnA = new RTCPeerConnection(configuration);\n        peerConnA.addEventListener('icecandidate',onIceCandidateA);// A发送candidate给B\n\n        peerConnB = new RTCPeerConnection(configuration);\n        peerConnB.addEventListener('icecandidate',onIceCandidateB);// B发送candidate给A\n\n        peerConnA.addEventListener('iceconnectionstatechange',onIceStateChangeA);//监听状态变化\n        peerConnB.addEventListener('iceconnectionstatechange',onIceStateChangeB);\n\n        peerConnB.addEventListener('track',gotRemoteStream);//获取到远端流以后\n\n        localStream.getTracks().forEach((track) => {//将流添加进\n            peerConnA.addTrack(track,localStream);\n        });\n\n        try{\n            // A生成offer提议，希望B应答\n            const offer = await peerConnA.createOffer();\n            await onCreateOfferSuccess(offer);\n\n        }catch(e){\n            onCreateSessionDescriptionError(e);\n        }\n\n    }\n\n\n\n    const onCreateOfferSuccess =  async (desc) => {\n        /* \n        提议/应答输出的是SDP信息，即媒体协商信息，如分辨率、格式、编码、加密算法等。\n        由于SDP的内容非常多，所以这里只展示了部分数据。SDP信息交换通过信息服务器完成。\n         */\n        // console.log(`peerConnA创建Offer返回的SDP信息\\n${desc.sdp}`);\n        console.log('设置peerConnA的本地描述start');\n        try{\n            // 设置本地描述\n           await peerConnA.setLocalDescription(desc);\n           onSetLocalSuccess(peerConnA);\n\n        }catch(e){\n            onSetSessionDescriptionError(e);\n        }\n\n        try{\n            // B设置远程描述\n            await peerConnB.setRemoteDescription(desc);\n            onSetRemoteSuccess(peerConnB);\n \n         }catch(e){\n            onSetSessionDescriptionError(e);\n         }\n\n\n\n         try{\n            //  B生成Answer应答\n            const answer = await peerConnB.createAnswer();\n            onCreateAnswerSuccess(answer);\n \n         }catch(e){\n            onCreateSessionDescriptionError(e);\n         }\n\n\n    }\n\n\n    const onCreateAnswerSuccess =  async (desc) => {\n        console.log(`peerConnB的应答Answer数据:\\n${desc.sdp}`);\n        console.log('peerConnB设置本地描述开始:setLocalDescription');\n        try{\n            // B设置本地描述\n            await peerConnB.setLocalDescription(desc);\n            onSetLocalSuccess(peerConnB);\n \n         }catch(e){\n            onSetSessionDescriptionError(e);\n         }\n \n\n         try{\n            //  A生成远端描述\n             await peerConnA.setRemoteDescription(desc);\n             onSetRemoteSuccess(peerConnA);\n  \n          }catch(e){\n            onSetSessionDescriptionError(e);\n          }\n\n    }\n\n\n    const onIceStateChangeA = (event) =>{\n        console.log(`peerConnA连接的ICE状态: ${peerConnA.iceConnectionState}`);\n        console.log('ICE状态改变事件: ', event);\n    }\n\n    const onIceStateChangeB = (event) =>{\n        console.log(`peerConnB连接的ICE状态: ${peerConnA.iceConnectionState}`);\n        console.log('ICE状态改变事件: ', event);\n    }\n\n\n    const onSetLocalSuccess = (pc) => {\n        console.log(`${getName(pc)}设置本地描述完成:setLocalDescription`);\n    }\n\n\n    const onSetRemoteSuccess = (pc) => {\n        console.log(`${getName(pc)}设置远端描述完成:setRemoteDescription`);\n    }\n\n    const getName = (pc) =>{\n        return (pc === peerConnA)?'peerConnA':'peerConnB';\n    }\n\n    const onCreateSessionDescriptionError = (error) => {\n        console.log(`创建会话描述SD错误: ${error.toString()}`);\n    }\n\n    const onSetSessionDescriptionError = (error) => {\n        console.log(`设置会话描述SD错误: ${error.toString()}`);\n    }\n\n    // A添加candidate\n    const onIceCandidateA = async (event) => {\n        try{\n            if(event.candidate){\n                await peerConnB.addIceCandidate(event.candidate);\n                onAddIceCandidateSuccess(peerConnB);\n            }\n        } catch(e){\n            onAddIceCandidateError(peerConnA, e);\n        }\n        /* \n        建立起连接后，除了可以看到媒体协商信息SDP，还可以看到网络协商信息Candidate，主要包含IP及端口信息。\n        端口是不断变化的\n        */\n        console.log(`IceCandidate数据:\\n${event.candidate ? event.candidate.candidate : '(null)'}`);\n    }\n    // B添加candidate\n    const onIceCandidateB = async (event) => {\n        try{\n            if(event.candidate){\n                await peerConnA.addIceCandidate(event.candidate);\n                onAddIceCandidateSuccess(peerConnA);\n            }\n        } catch(e){\n            onAddIceCandidateError(peerConnB, e);\n        }\n        console.log(`IceCandidate数据:\\n${event.candidate ? event.candidate.candidate : '(null)'}`);\n    }\n\n    const onAddIceCandidateSuccess = (pc) => {\n        console.log(`${getName(pc)}添加IceCandidate成功`);\n    }\n\n    const onAddIceCandidateError = (pc,error) =>{\n        console.log(`${getName(pc)}添加IceCandidate失败: ${error.toString()}`);\n    }\n\n\n\n\n    const gotRemoteStream = (e) =>{\n        // 远端流加入 本地流\n        if(remoteVideo.srcObject !== e.streams[0]){\n            remoteVideo.srcObject = e.streams[0];\n        }\n    }\n\n    const hangup = () =>{\n        console.log('结束会话');\n        peerConnA.close();\n        peerConnB.close();\n        peerConnA = null;\n        peerConnB = null;\n    }\n\n\n        return(\n            <div className=\"container\">\n            <h1>\n              <span>示例</span>\n            </h1>\n            <video  ref={localVideoRef} controls loop muted playsInline onCanPlay={canPlay}>\n                <source src=\"./assets/webrtc.mp4\" type=\"video/mp4\"/>\n            </video>\n            <video  ref={remoteVideoRef}  playsInline autoPlay></video>\n            <div>\n                <Button onClick={call}>呼叫</Button>\n                <Button onClick={hangup}>挂断</Button>\n            </div>\n          </div>   \n        );\n\n}\n\nexport default PeerConnectionVideo;","E:\\test\\webrtcjake\\src\\Examples\\DataChannelFile.jsx",["163","164"],"import React, { useRef, useEffect } from 'react'\nimport { Button } from 'antd'\n\nlet localConnection\nlet remoteConnection\nlet sendChannel\nlet receiveChannel\n\nlet fileReader\nlet receiveBuffer = []\nlet receivedSize = 0\nlet filetInput\nlet sendProgress\nlet receiveProgress\n\nconst DataChannelFile = () => {\n  const sendProgressRef = useRef()\n  const receiveProgressRef = useRef()\n  const fileInputRef = useRef()\n  const downloadRef = useRef()\n\n\n  useEffect(() => {\n    sendProgress = sendProgressRef.current\n    receiveProgress = receiveProgressRef.current\n\n    filetInput = fileInputRef.current\n    filetInput.addEventListener('change', async () => {\n      const file = filetInput.files[0]\n      if (!file) {\n        console.log('没有选择文件')\n      } else {\n        console.log('选择的文件是:' + file.name)\n      }\n    })\n    return () => {\n      \n    }\n  }, [])\n\n\n  const startSendFile = async () => {\n    localConnection = new RTCPeerConnection()\n    localConnection.addEventListener('icecandidate', onLocalIceCandidate)\n\n    sendChannel = localConnection.createDataChannel('webrtc-datachannel')\n    // 注意这里是arraybuffer\n    sendChannel.binaryType = 'arraybuffer'\n    sendChannel.onopen = onSendChannelStateChange\n    sendChannel.onclose = onSendChannelStateChange\n\n    remoteConnection = new RTCPeerConnection()\n    remoteConnection.addEventListener('icecandidate', onRemoteIceCandidate)\n\n    localConnection.addEventListener(\n      'iceconnectionstatechange',\n      onLocalIceStateChange\n    )\n    remoteConnection.addEventListener(\n      'iceconnectionstatechange',\n      onRemoteIceStateChange\n    )\n\n    remoteConnection.ondatachannel = receiveChannelCallback\n\n    try {\n      const offer = await localConnection.createOffer()\n      await onCreateOfferSuccess(offer)\n    } catch (e) {\n      onCreateSessionDescriptionError(e)\n    }\n  }\n\n  const onSendChannelStateChange = () => {\n    // 判断通道是否建立\n    const readyState = sendChannel.readyState\n    console.log('发送通道状态:' + readyState)\n\n    if (readyState === 'open') {\n      sendData()\n    }\n  }\n\n  const onReceiveChannelStateChange = () => {\n    const readyState = receiveChannel.readyState\n    console.log('接收通道状态:' + readyState)\n  }\n\n  const receiveChannelCallback = (event) => {\n    receiveChannel = event.channel\n    // 接受类型 定义\n    receiveChannel.binaryType = 'arraybuffer'\n    receiveChannel.onmessage = onReceiveMessageCallBack\n    receiveChannel.onopen = onReceiveChannelStateChange\n    receiveChannel.onclose = onReceiveChannelStateChange\n\n    receivedSize = 0   //每次创建都清零\n  }\n\n  const onReceiveMessageCallBack = (event) => {\n    //  缓存处理\n    receiveBuffer.push(event.data)\n    receivedSize += event.data.byteLength\n    receiveProgress.value = receivedSize\n\n    const file = filetInput.files[0]\n    // 接收完成\n    if (receivedSize === file.size) {\n      const received = new Blob(receiveBuffer)\n    //   缓存清空\n      receiveBuffer = []\n\n      let download = downloadRef.current\n      download.href = URL.createObjectURL(received)\n      download.download = file.name\n      download.textContent = `点击下载'${file.name}'&(${file.size} bytes)`\n      download.style.display = 'block'\n    }\n  }\n\n  const onCreateOfferSuccess = async (desc) => {\n    console.log(`localConnection创建Offer返回的SDP信息\\n${desc.sdp}`)\n    console.log('设置localConnection的本地描述start')\n    try {\n      await localConnection.setLocalDescription(desc)\n      onSetLocalSuccess(localConnection)\n    } catch (e) {\n      onSetSessionDescriptionError(e)\n    }\n\n    try {\n      await remoteConnection.setRemoteDescription(desc)\n      onSetRemoteSuccess(remoteConnection)\n    } catch (e) {\n      onSetSessionDescriptionError(e)\n    }\n\n    try {\n      const answer = await remoteConnection.createAnswer()\n      onCreateAnswerSuccess(answer)\n    } catch (e) {\n      onCreateSessionDescriptionError(e)\n    }\n  }\n\n  const onCreateAnswerSuccess = async (desc) => {\n    console.log(`remoteConnection的应答Answer数据:\\n${desc.sdp}`)\n    console.log('remoteConnection设置本地描述开始:setLocalDescription')\n    try {\n      await remoteConnection.setLocalDescription(desc)\n      onSetLocalSuccess(remoteConnection)\n    } catch (e) {\n      onSetSessionDescriptionError(e)\n    }\n\n    try {\n      await localConnection.setRemoteDescription(desc)\n      onSetRemoteSuccess(localConnection)\n    } catch (e) {\n      onSetSessionDescriptionError(e)\n    }\n  }\n\n  const onLocalIceStateChange = (event) => {\n    console.log(\n      `localConnection连接的ICE状态: ${localConnection.iceConnectionState}`\n    )\n    console.log('ICE状态改变事件: ', event)\n  }\n\n  const onRemoteIceStateChange = (event) => {\n    console.log(\n      `remoteConnection连接的ICE状态: ${localConnection.iceConnectionState}`\n    )\n    console.log('ICE状态改变事件: ', event)\n  }\n\n  const onSetLocalSuccess = (pc) => {\n    console.log(`${getName(pc)}设置本地描述完成:setLocalDescription`)\n  }\n\n  const onSetRemoteSuccess = (pc) => {\n    console.log(`${getName(pc)}设置远端描述完成:setRemoteDescription`)\n  }\n\n  const getName = (pc) => {\n    return pc === localConnection ? 'localConnection' : 'remoteConnection'\n  }\n\n  const onCreateSessionDescriptionError = (error) => {\n    console.log(`创建会话描述SD错误: ${error.toString()}`)\n  }\n\n  const onSetSessionDescriptionError = (error) => {\n    console.log(`设置会话描述SD错误: ${error.toString()}`)\n  }\n\n  const onLocalIceCandidate = async (event) => {\n    try {\n      if (event.candidate) {\n        await remoteConnection.addIceCandidate(event.candidate)\n        onAddIceCandidateSuccess(remoteConnection)\n      }\n    } catch (e) {\n      onAddIceCandidateError(localConnection, e)\n    }\n    console.log(\n      `IceCandidate数据:\\n${\n        event.candidate ? event.candidate.candidate : '(null)'\n      }`\n    )\n  }\n\n  const onRemoteIceCandidate = async (event) => {\n    try {\n      if (event.candidate) {\n        await localConnection.addIceCandidate(event.candidate)\n        onAddIceCandidateSuccess(localConnection)\n      }\n    } catch (e) {\n      onAddIceCandidateError(remoteConnection, e)\n    }\n    console.log(\n      `IceCandidate数据:\\n${\n        event.candidate ? event.candidate.candidate : '(null)'\n      }`\n    )\n  }\n\n  const onAddIceCandidateSuccess = (pc) => {\n    console.log(`${getName(pc)}添加IceCandidate成功`)\n  }\n\n  const onAddIceCandidateError = (pc, error) => {\n    console.log(`${getName(pc)}添加IceCandidate失败: ${error.toString()}`)\n  }\n\n  const sendData = () => {\n    let file = filetInput.files[0]\n    // progress bar\n    sendProgress.max = file.size\n    receiveProgress.max = file.size\n    // 暂设置16k，自由定义\n    let chunkSize = 16384\n    fileReader = new FileReader()\n    let offset = 0\n\n    fileReader.addEventListener('error', (error) => {\n      console.error('读取文件出错:', error)\n    })\n\n    fileReader.addEventListener('abort', (event) => {\n      console.error('读取文件取消:', event)\n    })\n\n    // 加载\n    fileReader.addEventListener('load', (e) => {\n      sendChannel.send(e.target.result)\n\n      offset += e.target.result.byteLength\n\n      sendProgress.value = offset\n    // 继续读取\n      if (offset < file.size) {\n        readSlice(offset)\n      }\n    })\n\n    // 读取切片\n    let readSlice = (o) => {\n      let slice = file.slice(offset, o + chunkSize)\n      fileReader.readAsArrayBuffer(slice)\n    }\n    readSlice(0) //读取切片从0开始\n  }\n\n  const closeChannel = () => {\n    console.log('结束会话')\n    sendChannel.close()\n    if (receiveChannel) {\n      receiveChannel.close()\n    }\n    localConnection.close()\n    remoteConnection.close()\n    localConnection = null\n    remoteConnection = null\n  }\n\n  const cancelSendFile = () => {\n    if (fileReader && fileReader.readyState === 1) {\n      console.log('取消读取文件')\n      fileReader.abort()\n    }\n  }\n\n  return (\n    <div className=\"container\">\n      <h1>\n        <span>示例</span>\n      </h1>\n      <div>\n        <form id=\"fileInfo\">\n          <input type=\"file\" ref={fileInputRef} name=\"files\" />\n        </form>\n        <div>\n          <h2>发送</h2>\n          <progress\n            ref={sendProgressRef}\n            max=\"0\"\n            value=\"0\"\n            style={{ width: '500px' }}\n          ></progress>\n        </div>\n        <div>\n          <h2>接收</h2>\n          <progress\n            ref={receiveProgressRef}\n            max=\"0\"\n            value=\"0\"\n            style={{ width: '500px' }}\n          ></progress>\n        </div>\n      </div>\n\n      <a ref={downloadRef}></a>\n      <Button onClick={startSendFile}>发送</Button>\n      <Button onClick={cancelSendFile}>取消</Button>\n      <Button onClick={closeChannel}>关闭</Button>\n    </div>\n  )\n}\n\nexport default DataChannelFile\n","E:\\test\\webrtcjake\\src\\Examples\\PeerConnectionCanvas.jsx",["165","166","167","168"],"import React,{useState,useRef,useEffect} from \"react\";\nimport {Button,message} from \"antd\";\nimport '../styles/css/pc-canvas.scss';\n\n\nlet canvas;\nlet context;\n\nlet localVideo;\nlet remoteVideo;\nlet localStream;\nlet peerConnA;\nlet peerConnB;\n\nconst PeerConnectionCanvas =()=>{\n    const remoteVideoRef=useRef()\n    const canvasRef=useRef()\n\n    useEffect(() => {\n        remoteVideo = remoteVideoRef.current;\n        canvas =canvasRef.current;\n        startCaptureCanvas();\n        return () => {\n           \n        };\n    }, []);\n\n\n    const startCaptureCanvas = async (e) => {\n        // 抓取canvas流\n        localStream = canvas.captureStream(10);\n    \n        drawLine();\n    }\n\n    const drawLine = () => {\n        context = canvas.getContext('2d');\n\n        context.fillStyle = '#CCC';\n        context.fillRect(0,0,320,240);\n\n        context.lineWidth = 1;\n        context.strokeStyle = \"#FF0000\";\n\n        canvas.addEventListener(\"mousedown\",startAction);\n        canvas.addEventListener(\"mouseup\",endAction);\n    }\n\n    const startAction = (event) => {\n        context.beginPath();\n        context.moveTo(event.offsetX,event.offsetY);\n        context.stroke();\n\n        canvas.addEventListener(\"mousemove\",moveAction);\n    }\n\n\n    const moveAction = (event) => {\n        context.lineTo(event.offsetX,event.offsetY);\n        context.stroke();\n    }\n\n    const endAction = (event) => {\n        canvas.removeEventListener(\"mousemove\",moveAction);\n    }\n\n    \n\n    const call = async () => {\n\n        //视频轨道\n        const videoTracks = localStream.getVideoTracks();\n        //音频轨道\n        const audioTracks = localStream.getAudioTracks();\n        //判断视频轨道是否有值\n        if (videoTracks.length > 0) {\n        //输出摄像头的名称\n        console.log(`使用的视频设备为: ${videoTracks[0].label}`);\n        }\n        //判断音频轨道是否有值\n        if (audioTracks.length > 0) {\n        //输出麦克风的名称\n        console.log(`使用的音频设备为: ${audioTracks[0].label}`);\n        }\n        // 设置iceServer, 使用google服务器\n        let configuration = {\"iceServers\":[{ \"url\": \"stun:stun.l.google.com:19302\" }]};\n\n        peerConnA = new RTCPeerConnection(configuration);\n        peerConnA.addEventListener('icecandidate',onIceCandidateA);// A发送candidate给B\n\n        peerConnB = new RTCPeerConnection(configuration);\n        peerConnB.addEventListener('icecandidate',onIceCandidateB);// B发送candidate给A\n\n        peerConnA.addEventListener('iceconnectionstatechange',onIceStateChangeA);//监听状态变化\n        peerConnB.addEventListener('iceconnectionstatechange',onIceStateChangeB);\n\n        peerConnB.addEventListener('track',gotRemoteStream);//获取到远端流以后\n\n        localStream.getTracks().forEach((track) => {//将流添加进\n            peerConnA.addTrack(track,localStream);\n        });\n\n        try{\n            // A生成offer提议，希望B应答\n            const offer = await peerConnA.createOffer();\n            await onCreateOfferSuccess(offer);\n\n        }catch(e){\n            onCreateSessionDescriptionError(e);\n        }\n\n    }\n\n\n    const onCreateOfferSuccess =  async (desc) => {\n        /* \n        提议/应答输出的是SDP信息，即媒体协商信息，如分辨率、格式、编码、加密算法等。\n        由于SDP的内容非常多，所以这里只展示了部分数据。SDP信息交换通过信息服务器完成。\n         */\n        console.log(`peerConnA创建Offer返回的SDP信息\\n${desc.sdp}`);\n        console.log('设置peerConnA的本地描述start');\n        try{\n            // 设置本地描述\n           await peerConnA.setLocalDescription(desc);\n           onSetLocalSuccess(peerConnA);\n\n        }catch(e){\n            onSetSessionDescriptionError(e);\n        }\n\n        try{\n            // B设置远程描述\n            await peerConnB.setRemoteDescription(desc);\n            onSetRemoteSuccess(peerConnB);\n \n         }catch(e){\n            onSetSessionDescriptionError(e);\n         }\n\n\n\n         try{\n            //  B生成Answer应答\n            const answer = await peerConnB.createAnswer();\n            onCreateAnswerSuccess(answer);\n \n         }catch(e){\n            onCreateSessionDescriptionError(e);\n         }\n\n\n    }\n\n\n    const onCreateAnswerSuccess =  async (desc) => {\n        console.log(`peerConnB的应答Answer数据:\\n${desc.sdp}`);\n        console.log('peerConnB设置本地描述开始:setLocalDescription');\n        try{\n            // B设置本地描述\n            await peerConnB.setLocalDescription(desc);\n            onSetLocalSuccess(peerConnB);\n \n         }catch(e){\n            onSetSessionDescriptionError(e);\n         }\n \n\n         try{\n            //  A生成远端描述\n             await peerConnA.setRemoteDescription(desc);\n             onSetRemoteSuccess(peerConnA);\n  \n          }catch(e){\n            onSetSessionDescriptionError(e);\n          }\n\n    }\n\n    const onIceStateChangeA = (event) =>{\n        console.log(`peerConnA连接的ICE状态: ${peerConnA.iceConnectionState}`);\n        console.log('ICE状态改变事件: ', event);\n    }\n\n    const onIceStateChangeB = (event) =>{\n        console.log(`peerConnB连接的ICE状态: ${peerConnA.iceConnectionState}`);\n        console.log('ICE状态改变事件: ', event);\n    }\n\n\n    const onSetLocalSuccess = (pc) => {\n        console.log(`${getName(pc)}设置本地描述完成:setLocalDescription`);\n    }\n\n\n    const onSetRemoteSuccess = (pc) => {\n        console.log(`${getName(pc)}设置远端描述完成:setRemoteDescription`);\n    }\n\n    const getName = (pc) =>{\n        return (pc === peerConnA)?'peerConnA':'peerConnB';\n    }\n\n    const onCreateSessionDescriptionError = (error) => {\n        console.log(`创建会话描述SD错误: ${error.toString()}`);\n    }\n\n    const onSetSessionDescriptionError = (error) => {\n        console.log(`设置会话描述SD错误: ${error.toString()}`);\n    }\n\n    // A添加candidate\n    const onIceCandidateA = async (event) => {\n        try{\n            if(event.candidate){\n                await peerConnB.addIceCandidate(event.candidate);\n                onAddIceCandidateSuccess(peerConnB);\n            }\n        } catch(e){\n            onAddIceCandidateError(peerConnA, e);\n        }\n        /* \n        建立起连接后，除了可以看到媒体协商信息SDP，还可以看到网络协商信息Candidate，主要包含IP及端口信息。\n        端口是不断变化的\n        */\n        console.log(`IceCandidate数据:\\n${event.candidate ? event.candidate.candidate : '(null)'}`);\n    }\n    // B添加candidate\n    const onIceCandidateB = async (event) => {\n        try{\n            if(event.candidate){\n                await peerConnA.addIceCandidate(event.candidate);\n                onAddIceCandidateSuccess(peerConnA);\n            }\n        } catch(e){\n            onAddIceCandidateError(peerConnB, e);\n        }\n        console.log(`IceCandidate数据:\\n${event.candidate ? event.candidate.candidate : '(null)'}`);\n    }\n\n    const onAddIceCandidateSuccess = (pc) => {\n        console.log(`${getName(pc)}添加IceCandidate成功`);\n    }\n\n    const onAddIceCandidateError = (pc,error) =>{\n        console.log(`${getName(pc)}添加IceCandidate失败: ${error.toString()}`);\n    }\n\n\n\n\n    const gotRemoteStream = (e) =>{\n        // 远端流加入 本地流\n        if(remoteVideo.srcObject !== e.streams[0]){\n            remoteVideo.srcObject = e.streams[0];\n        }\n    }\n\n    const hangup = () =>{\n        console.log('结束会话');\n        peerConnA.close();\n        peerConnB.close();\n        peerConnA = null;\n        peerConnB = null;\n    }\n\n\n        return(\n            <div className=\"container\">\n            <h1>\n              <span>示例</span>\n            </h1>\n            <div className=\"small-canvas\">\n                <canvas ref={canvasRef}></canvas>\n            </div>\n            <video className=\"small-video\" ref={remoteVideoRef}  playsInline autoPlay></video>\n            <div>\n                <Button onClick={call}>呼叫</Button>\n                <Button onClick={hangup}>挂断</Button>\n            </div>\n          </div>   \n        );\n\n}\n\nexport default PeerConnectionCanvas;","E:\\test\\webrtcjake\\src\\Examples\\PeerConnection.jsx",["169","170"],"import React,{useState,useRef,useEffect} from \"react\";\nimport {Button,message} from \"antd\";\n\n\nlet localVideo;\nlet remoteVideo;\nlet localStream;\nlet peerConnA;\nlet peerConnB;\n\nconst PeerConnection=()=>{\n    const localVideoRef=useRef();\n    const remoteVideoRef=useRef();\n\n    useEffect(()=>{\n        localVideo = localVideoRef.current\n        remoteVideo = remoteVideoRef.current\n        //视频清晰度自适应 \n        localVideo.addEventListener('loadedmetadata',() => {\n            console.log(\"本地视频尺寸为:\" + localVideo.videoWidth + \" \" + localVideo.videoHeight);\n        });\n\n        remoteVideo.addEventListener('loadedmetadata',() => {\n            console.log(\"远端视频尺寸为:\" + remoteVideo.videoWidth + \" \" + remoteVideo.videoHeight);\n        });\n\n        remoteVideo.addEventListener('resize',() => {\n            console.log(\"远端视频尺寸为:\" + remoteVideo.videoWidth + \" \" + remoteVideo.videoHeight);\n        });\n    },[])\n    \n\n    const start = async () =>{\n        try{\n            const stream = await navigator.mediaDevices.getUserMedia({audio:true,video:true});\n            localVideo.srcObject = stream;\n            localStream = stream;\n        }catch(e){\n            console.log(\"getUserMedia错误:\" + e);\n        }\n    }\n    \n    const call = async () => {\n\n        //视频轨道\n        const videoTracks = localStream.getVideoTracks();\n        //音频轨道\n        const audioTracks = localStream.getAudioTracks();\n        //判断视频轨道是否有值\n        if (videoTracks.length > 0) {\n        //输出摄像头的名称\n        console.log(`使用的视频设备为: ${videoTracks[0].label}`);\n        }\n        //判断音频轨道是否有值\n        if (audioTracks.length > 0) {\n        //输出麦克风的名称\n        console.log(`使用的音频设备为: ${audioTracks[0].label}`);\n        }\n        // 设置iceServer, 使用google服务器\n        let configuration = {\"iceServers\":[{ \"url\": \"stun:stun.l.google.com:19302\" }]};\n\n        peerConnA = new RTCPeerConnection(configuration);\n        peerConnA.addEventListener('icecandidate',onIceCandidateA);// A发送candidate给B\n\n        peerConnB = new RTCPeerConnection(configuration);\n        peerConnB.addEventListener('icecandidate',onIceCandidateB);// B发送candidate给A\n\n        peerConnA.addEventListener('iceconnectionstatechange',onIceStateChangeA);//监听状态变化\n        peerConnB.addEventListener('iceconnectionstatechange',onIceStateChangeB);\n\n        peerConnB.addEventListener('track',gotRemoteStream);//获取到远端流以后\n\n        localStream.getTracks().forEach((track) => {//将流添加进\n            peerConnA.addTrack(track,localStream);\n        });\n\n        try{\n            // A生成offer提议，希望B应答\n            const offer = await peerConnA.createOffer();\n            await onCreateOfferSuccess(offer);\n\n        }catch(e){\n            onCreateSessionDescriptionError(e);\n        }\n\n    }\n\n\n    const onCreateOfferSuccess =  async (desc) => {\n        /* \n        提议/应答输出的是SDP信息，即媒体协商信息，如分辨率、格式、编码、加密算法等。\n        由于SDP的内容非常多，所以这里只展示了部分数据。SDP信息交换通过信息服务器完成。\n         */\n        console.log(`peerConnA创建Offer返回的SDP信息\\n${desc.sdp}`);\n        console.log('设置peerConnA的本地描述start');\n        try{\n            // 设置本地描述\n           await peerConnA.setLocalDescription(desc);\n           onSetLocalSuccess(peerConnA);\n\n        }catch(e){\n            onSetSessionDescriptionError(e);\n        }\n\n        try{\n            // B设置远程描述\n            await peerConnB.setRemoteDescription(desc);\n            onSetRemoteSuccess(peerConnB);\n \n         }catch(e){\n            onSetSessionDescriptionError(e);\n         }\n\n\n\n         try{\n            //  B生成Answer应答\n            const answer = await peerConnB.createAnswer();\n            onCreateAnswerSuccess(answer);\n \n         }catch(e){\n            onCreateSessionDescriptionError(e);\n         }\n\n\n    }\n\n\n    const onCreateAnswerSuccess =  async (desc) => {\n        console.log(`peerConnB的应答Answer数据:\\n${desc.sdp}`);\n        console.log('peerConnB设置本地描述开始:setLocalDescription');\n        try{\n            // B设置本地描述\n            await peerConnB.setLocalDescription(desc);\n            onSetLocalSuccess(peerConnB);\n \n         }catch(e){\n            onSetSessionDescriptionError(e);\n         }\n \n\n         try{\n            //  A生成远端描述\n             await peerConnA.setRemoteDescription(desc);\n             onSetRemoteSuccess(peerConnA);\n  \n          }catch(e){\n            onSetSessionDescriptionError(e);\n          }\n\n    }\n\n    const onIceStateChangeA = (event) =>{\n        console.log(`peerConnA连接的ICE状态: ${peerConnA.iceConnectionState}`);\n        console.log('ICE状态改变事件: ', event);\n    }\n\n    const onIceStateChangeB = (event) =>{\n        console.log(`peerConnB连接的ICE状态: ${peerConnA.iceConnectionState}`);\n        console.log('ICE状态改变事件: ', event);\n    }\n\n\n    const onSetLocalSuccess = (pc) => {\n        console.log(`${getName(pc)}设置本地描述完成:setLocalDescription`);\n    }\n\n\n    const onSetRemoteSuccess = (pc) => {\n        console.log(`${getName(pc)}设置远端描述完成:setRemoteDescription`);\n    }\n\n    const getName = (pc) =>{\n        return (pc === peerConnA)?'peerConnA':'peerConnB';\n    }\n\n    const onCreateSessionDescriptionError = (error) => {\n        console.log(`创建会话描述SD错误: ${error.toString()}`);\n    }\n\n    const onSetSessionDescriptionError = (error) => {\n        console.log(`设置会话描述SD错误: ${error.toString()}`);\n    }\n\n    // A添加candidate\n    const onIceCandidateA = async (event) => {\n        try{\n            if(event.candidate){\n                await peerConnB.addIceCandidate(event.candidate);\n                onAddIceCandidateSuccess(peerConnB);\n            }\n        } catch(e){\n            onAddIceCandidateError(peerConnA, e);\n        }\n        /* \n        建立起连接后，除了可以看到媒体协商信息SDP，还可以看到网络协商信息Candidate，主要包含IP及端口信息。\n        端口是不断变化的\n        */\n        console.log(`IceCandidate数据:\\n${event.candidate ? event.candidate.candidate : '(null)'}`);\n    }\n    // B添加candidate\n    const onIceCandidateB = async (event) => {\n        try{\n            if(event.candidate){\n                await peerConnA.addIceCandidate(event.candidate);\n                onAddIceCandidateSuccess(peerConnA);\n            }\n        } catch(e){\n            onAddIceCandidateError(peerConnB, e);\n        }\n        console.log(`IceCandidate数据:\\n${event.candidate ? event.candidate.candidate : '(null)'}`);\n    }\n\n    const onAddIceCandidateSuccess = (pc) => {\n        console.log(`${getName(pc)}添加IceCandidate成功`);\n    }\n\n    const onAddIceCandidateError = (pc,error) =>{\n        console.log(`${getName(pc)}添加IceCandidate失败: ${error.toString()}`);\n    }\n\n\n\n\n    const gotRemoteStream = (e) =>{\n        // 远端流加入 本地流\n        if(remoteVideo.srcObject !== e.streams[0]){\n            remoteVideo.srcObject = e.streams[0];\n        }\n    }\n\n    const hangup = () =>{\n        console.log('结束会话');\n        peerConnA.close();\n        peerConnB.close();\n        peerConnA = null;\n        peerConnB = null;\n    }\n\n\n        return(\n            <div className=\"container\">\n            <h1>\n              <span>示例</span>\n            </h1>\n            <video ref={localVideoRef}  playsInline autoPlay muted></video>\n            <video ref={remoteVideoRef}  playsInline autoPlay></video>\n            <Button onClick={start}>开始</Button>\n            <Button onClick={call}>呼叫</Button>\n            <Button onClick={hangup}>挂断</Button>\n          </div>   \n        );\n\n}\n\nexport default PeerConnection;","E:\\test\\webrtcjake\\src\\Examples\\RecordCanvas.jsx",["171","172","173"],"import React, { useState, useEffect, useRef } from 'react'\nimport { Button, message } from 'antd'\nimport '../styles/css/record-canvas.scss'\n\nlet mediaRecorder\n\nlet recordedBlobs\n\nlet stream\nlet canvas\nlet context\n\nconst RecordCanvas = () => {\n  const canvasRef = useRef()\n  const videoRef = useRef()\n\n  useEffect(() => {\n    drawLine()\n  }, [])\n\n  const drawLine = () => {\n    canvas = canvasRef.current\n    context = canvas.getContext('2d')\n\n    context.fillStyle = '#ececec'\n    context.fillRect(0, 0, 320, 320)\n\n    context.lineWidth = 1\n    context.strokeStyle = '#28d5de'\n\n    canvas.addEventListener('mousedown', startAction)\n    canvas.addEventListener('mouseup', endAction)\n  }\n\n  const startAction = (event) => {\n    context.beginPath()\n    context.moveTo(event.offsetX, event.offsetY)\n    context.stroke()\n\n    canvas.addEventListener('mousemove', moveAction)\n  }\n\n  const moveAction = (event) => {\n    context.lineTo(event.offsetX, event.offsetY)\n    context.stroke()\n  }\n\n  const endAction = (event) => {\n    canvas.removeEventListener('mousemove', moveAction)\n  }\n\n  const startCaptureCanvas = async (e) => {\n    stream = canvas.captureStream(10)\n    const video = videoRef.current\n    window.stream = stream\n    video.srcObject = stream\n\n    startRecord()\n  }\n\n  const startRecord = (e) => {\n    stream.addEventListener('inactive', (e) => {\n      stopRecord(e)\n    })\n\n    recordedBlobs = []\n\n    try {\n      mediaRecorder = new MediaRecorder(window.stream, {\n        mineType: 'video/webm',\n      })\n    } catch (e) {\n      console.log('MediaRecorder:', e)\n      return\n    }\n\n    mediaRecorder.onstop = (event) => {\n      console.log('Recorder stopped:', event)\n      console.log('Recorder blobs:', recordedBlobs)\n    }\n\n    mediaRecorder.ondataavailable = handleDataAvailable\n    mediaRecorder.start(10)\n  }\n\n  const stopRecord = (e) => {\n    try {\n      mediaRecorder.stop()\n    } catch (e) {\n      console.log(e)\n    }\n\n    const blob = new Blob(recordedBlobs, { type: 'video/webm' })\n    const url = window.URL.createObjectURL(blob)\n\n    const link = document.createElement('a')\n    link.style.display = 'none'\n    link.href = url\n    link.download = 'canvas.webm'\n    document.body.appendChild(link)\n    link.click()\n    setTimeout(() => {\n      document.body.removeChild(link)\n      window.URL.revokeObjectURL(url)\n    }, 100)\n  }\n  const handleDataAvailable = (event) => {\n    if (event.data && event.data.size > 0) {\n      recordedBlobs.push(event.data)\n    }\n  }\n\n  return (\n    <div className=\"container\">\n      <h1>\n        <span>示例</span>\n      </h1>\n\n      <div className=\"small-canvas\">\n        <canvas ref={canvasRef}></canvas>\n      </div>\n      <video\n        className=\"small-video\"\n        ref={videoRef}\n        playsInline\n        autoPlay\n      ></video>\n      <div>\n        <Button className=\"button\" onClick={startCaptureCanvas}>\n          开始\n        </Button>\n        <Button className=\"button\" onClick={stopRecord}>\n          停止\n        </Button>\n      </div>\n    </div>\n  )\n}\n\nexport default RecordCanvas\n","E:\\test\\webrtcjake\\src\\Examples\\DataChannel.jsx",[],"E:\\test\\webrtcjake\\src\\Examples\\volume\\AudioVolume.jsx",["174"],"import React, { useRef, useEffect, useState } from 'react'\n// import {Button,message} from \"antd\";\nimport SoundMeter from './soundmeter'\n\nlet soundMeter\nconst AudioVolume = () => {\n  const [audioLevel, setAudioLevel] = useState(0)\n  const timer = useRef()\n  const timer2 = useRef()\n  useEffect(() => {\n    try {\n      window.AudioContext = window.AudioContext || window.webkitAudioContext\n      window.audioContext = new AudioContext()\n    } catch (e) {\n      console.log('网页音频API不支持.')\n    }\n\n    soundMeter = window.soundMeter = new SoundMeter(window.audioContext)\n\n    const constraints = (window.constraints = {\n      audio: true,\n      video: false,\n    })\n\n    navigator.mediaDevices\n      .getUserMedia(constraints)\n      .then(handleSuccess)\n      .catch(handleError)\n    return () => {\n      clearTimeout(timer)\n      clearTimeout(timer2)\n    }\n  }, [])\n\n  const handleSuccess = (stream) => {\n    window.stream = stream\n    soundMeter.connectToSource(stream)\n    timer.current = setTimeout(soundMeterProcess, 100)\n  }\n\n  const soundMeterProcess = () => {\n    let val = window.soundMeter.instant.toFixed(2) * 348 + 1\n    setAudioLevel(val)\n    timer2.current = setTimeout(soundMeterProcess, 100)\n  }\n\n  const handleError = (error) => {\n    console.log('getUserMedia error:', error.message, error.name)\n  }\n\n  return (\n    <div className=\"container\">\n      <h1>\n        <span>音量检测示例</span>\n      </h1>\n      <div\n        style={{\n          width: audioLevel + 'px',\n          height: '10px',\n          backgroundColor: '#8dc63f',\n          marginTop: '20px',\n        }}\n      ></div>\n    </div>\n  )\n}\n\nexport default AudioVolume\n","E:\\test\\webrtcjake\\src\\Examples\\media-settings\\soundmeter.js",["175","176"],"/*\n *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.\n *\n *  Use of this source code is governed by a BSD-style license\n *  that can be found in the LICENSE file in the root of the source\n *  tree.\n */\n\n'use strict';\n\n// Meter class that generates a number correlated to audio volume.\n// The meter class itself displays nothing, but it makes the\n// instantaneous and time-decaying volumes available for inspection.\n// It also reports on the fraction of samples that were at or near\n// the top of the measurement range.\nexport default class SoundMeter {\nconstructor(context) {\n  this.context = context;\n  this.instant = 0.0;\n  //this.slow = 0.0;\n  //this.clip = 0.0;\n  this.script = context.createScriptProcessor(2048, 1, 1);\n  var that = this;\n  this.script.onaudioprocess = function(event) {\n    var input = event.inputBuffer.getChannelData(0);\n    var i;\n    var sum = 0.0;\n    var clipcount = 0;\n    for (i = 0; i < input.length; ++i) {\n      sum += input[i] * input[i];\n      if (Math.abs(input[i]) > 0.99) {\n        clipcount += 1;\n      }\n    }\n    that.instant = Math.sqrt(sum / input.length);\n    //that.slow = 0.95 * that.slow + 0.05 * that.instant;\n    //that.clip = clipcount / input.length;\n  };\n}\n\nconnectToSource(stream) {\n  this.mic = this.context.createMediaStreamSource(stream);\n  this.mic.connect(this.script);\n  // necessary to make sample run, but should not be.\n  this.script.connect(this.context.destination);\n};\n\nstop() {\n  this.mic.disconnect();\n  this.script.disconnect();\n};\n}","E:\\test\\webrtcjake\\src\\Examples\\volume\\soundmeter.js",["177","178"],"/*\n *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.\n *\n *  Use of this source code is governed by a BSD-style license\n *  that can be found in the LICENSE file in the root of the source\n *  tree.\n */\n\n'use strict';\n\n// Meter class that generates a number correlated to audio volume.\n// The meter class itself displays nothing, but it makes the\n// instantaneous and time-decaying volumes available for inspection.\n// It also reports on the fraction of samples that were at or near\n// the top of the measurement range.\n// 第三方的测量音量的方法。\nexport default class SoundMeter {\nconstructor(context) {\n  this.context = context;\n  this.instant = 0.0;\n  //this.slow = 0.0;\n  //this.clip = 0.0;\n  this.script = context.createScriptProcessor(2048, 1, 1);\n  var that = this;\n  this.script.onaudioprocess = function(event) {\n    var input = event.inputBuffer.getChannelData(0);\n    var i;\n    var sum = 0.0;\n    var clipcount = 0;\n    for (i = 0; i < input.length; ++i) {\n      sum += input[i] * input[i];\n      if (Math.abs(input[i]) > 0.99) {\n        clipcount += 1;\n      }\n    }\n    that.instant = Math.sqrt(sum / input.length);\n    //that.slow = 0.95 * that.slow + 0.05 * that.instant;\n    //that.clip = clipcount / input.length;\n  };\n}\n\nconnectToSource(stream) {\n  this.mic = this.context.createMediaStreamSource(stream);\n  this.mic.connect(this.script);\n  // necessary to make sample run, but should not be.\n  this.script.connect(this.context.destination);\n};\n\nstop() {\n  this.mic.disconnect();\n  this.script.disconnect();\n};\n}",["179","180"],"E:\\test\\webrtcjake\\src\\router\\index.js",[],"E:\\test\\webrtcjake\\src\\Examples\\Error.jsx",["181"],"E:\\test\\webrtcjake\\src\\Examples\\CaptureCanvas.jsx",[],{"ruleId":"182","replacedBy":"183"},{"ruleId":"184","replacedBy":"185"},{"ruleId":"186","severity":1,"message":"187","line":68,"column":6,"nodeType":"188","endLine":68,"endColumn":8,"suggestions":"189"},{"ruleId":"186","severity":1,"message":"190","line":14,"column":7,"nodeType":"188","endLine":14,"endColumn":9,"suggestions":"191"},{"ruleId":"192","severity":1,"message":"193","line":2,"column":10,"nodeType":"194","messageId":"195","endLine":2,"endColumn":16},{"ruleId":"186","severity":1,"message":"196","line":15,"column":17,"nodeType":"197","endLine":15,"endColumn":33},{"ruleId":"186","severity":1,"message":"198","line":61,"column":3,"nodeType":"188","endLine":61,"endColumn":5,"suggestions":"199"},{"ruleId":"186","severity":1,"message":"190","line":17,"column":7,"nodeType":"188","endLine":17,"endColumn":9,"suggestions":"200"},{"ruleId":"186","severity":1,"message":"201","line":14,"column":3,"nodeType":"188","endLine":14,"endColumn":5,"suggestions":"202"},{"ruleId":"192","severity":1,"message":"203","line":2,"column":18,"nodeType":"194","messageId":"195","endLine":2,"endColumn":25},{"ruleId":"204","severity":1,"message":"205","line":143,"column":28,"nodeType":"206","messageId":"207","endLine":143,"endColumn":30},{"ruleId":"204","severity":1,"message":"205","line":150,"column":28,"nodeType":"206","messageId":"207","endLine":150,"endColumn":30},{"ruleId":"204","severity":1,"message":"205","line":157,"column":28,"nodeType":"206","messageId":"207","endLine":157,"endColumn":30},{"ruleId":"204","severity":1,"message":"205","line":164,"column":28,"nodeType":"206","messageId":"207","endLine":164,"endColumn":30},{"ruleId":"204","severity":1,"message":"205","line":171,"column":28,"nodeType":"206","messageId":"207","endLine":171,"endColumn":30},{"ruleId":"192","severity":1,"message":"203","line":2,"column":18,"nodeType":"194","messageId":"195","endLine":2,"endColumn":25},{"ruleId":"204","severity":1,"message":"205","line":101,"column":28,"nodeType":"206","messageId":"207","endLine":101,"endColumn":30},{"ruleId":"204","severity":1,"message":"205","line":108,"column":28,"nodeType":"206","messageId":"207","endLine":108,"endColumn":30},{"ruleId":"204","severity":1,"message":"205","line":115,"column":28,"nodeType":"206","messageId":"207","endLine":115,"endColumn":30},{"ruleId":"204","severity":1,"message":"205","line":122,"column":28,"nodeType":"206","messageId":"207","endLine":122,"endColumn":30},{"ruleId":"204","severity":1,"message":"205","line":129,"column":28,"nodeType":"206","messageId":"207","endLine":129,"endColumn":30},{"ruleId":"186","severity":1,"message":"187","line":39,"column":6,"nodeType":"188","endLine":39,"endColumn":8,"suggestions":"208"},{"ruleId":"192","severity":1,"message":"209","line":1,"column":22,"nodeType":"194","messageId":"195","endLine":1,"endColumn":31},{"ruleId":"192","severity":1,"message":"210","line":1,"column":32,"nodeType":"194","messageId":"195","endLine":1,"endColumn":40},{"ruleId":"192","severity":1,"message":"203","line":2,"column":18,"nodeType":"194","messageId":"195","endLine":2,"endColumn":25},{"ruleId":"192","severity":1,"message":"211","line":4,"column":7,"nodeType":"194","messageId":"195","endLine":4,"endColumn":18},{"ruleId":"192","severity":1,"message":"210","line":1,"column":15,"nodeType":"194","messageId":"195","endLine":1,"endColumn":23},{"ruleId":"192","severity":1,"message":"203","line":2,"column":16,"nodeType":"194","messageId":"195","endLine":2,"endColumn":23},{"ruleId":"212","severity":1,"message":"213","line":325,"column":7,"nodeType":"214","endLine":325,"endColumn":28},{"ruleId":"215","severity":1,"message":"216","line":325,"column":7,"nodeType":"214","endLine":325,"endColumn":28},{"ruleId":"192","severity":1,"message":"210","line":1,"column":15,"nodeType":"194","messageId":"195","endLine":1,"endColumn":23},{"ruleId":"192","severity":1,"message":"203","line":2,"column":16,"nodeType":"194","messageId":"195","endLine":2,"endColumn":23},{"ruleId":"192","severity":1,"message":"217","line":9,"column":5,"nodeType":"194","messageId":"195","endLine":9,"endColumn":15},{"ruleId":"186","severity":1,"message":"218","line":26,"column":8,"nodeType":"188","endLine":26,"endColumn":10,"suggestions":"219"},{"ruleId":"192","severity":1,"message":"210","line":1,"column":15,"nodeType":"194","messageId":"195","endLine":1,"endColumn":23},{"ruleId":"192","severity":1,"message":"203","line":2,"column":16,"nodeType":"194","messageId":"195","endLine":2,"endColumn":23},{"ruleId":"192","severity":1,"message":"210","line":1,"column":17,"nodeType":"194","messageId":"195","endLine":1,"endColumn":25},{"ruleId":"192","severity":1,"message":"203","line":2,"column":18,"nodeType":"194","messageId":"195","endLine":2,"endColumn":25},{"ruleId":"186","severity":1,"message":"220","line":19,"column":6,"nodeType":"188","endLine":19,"endColumn":8,"suggestions":"221"},{"ruleId":"186","severity":1,"message":"222","line":33,"column":6,"nodeType":"188","endLine":33,"endColumn":8,"suggestions":"223"},{"ruleId":"224","severity":1,"message":"225","line":9,"column":1,"nodeType":"226","messageId":"227","endLine":9,"endColumn":14,"fix":"228"},{"ruleId":"192","severity":1,"message":"229","line":32,"column":9,"nodeType":"194","messageId":"195","endLine":32,"endColumn":18},{"ruleId":"224","severity":1,"message":"225","line":9,"column":1,"nodeType":"226","messageId":"227","endLine":9,"endColumn":14,"fix":"230"},{"ruleId":"192","severity":1,"message":"229","line":33,"column":9,"nodeType":"194","messageId":"195","endLine":33,"endColumn":18},{"ruleId":"182","replacedBy":"231"},{"ruleId":"184","replacedBy":"232"},{"ruleId":"192","severity":1,"message":"233","line":4,"column":7,"nodeType":"194","messageId":"195","endLine":4,"endColumn":12},"no-native-reassign",["234"],"no-negated-in-lhs",["235"],"react-hooks/exhaustive-deps","React Hook useEffect has missing dependencies: 'selectedAudioDevice', 'selectedAudioOutputDevice', and 'selectedVideoDevice'. Either include them or remove the dependency array.","ArrayExpression",["236"],"React Hook useEffect has a missing dependency: 'openCamera'. Either include it or remove the dependency array.",["237"],"no-unused-vars","'Button' is defined but never used.","Identifier","unusedVar","Assignments to the 'video' variable from inside React Hook useEffect will be lost after each render. To preserve the value over time, store it in a useRef Hook and keep the mutable value in the '.current' property. Otherwise, you can move this variable directly inside useEffect.","MemberExpression","React Hook useEffect has a missing dependency: 'getMedia'. Either include it or remove the dependency array.",["238"],["239"],"React Hook useEffect has a missing dependency: 'openDevice'. Either include it or remove the dependency array.",["240"],"'message' is defined but never used.","eqeqeq","Expected '!==' and instead saw '!='.","BinaryExpression","unexpected",["241"],"'useEffect' is defined but never used.","'useState' is defined but never used.","'constraints' is assigned a value but never used.","jsx-a11y/anchor-has-content","Anchors must have content and the content must be accessible by a screen reader.","JSXOpeningElement","jsx-a11y/anchor-is-valid","The href attribute is required for an anchor to be keyboard accessible. Provide a valid, navigable address as the href value. If you cannot provide an href, but still need the element to resemble a link, use a button and change it with appropriate styles. Learn more: https://github.com/evcohen/eslint-plugin-jsx-a11y/blob/master/docs/rules/anchor-is-valid.md","'localVideo' is defined but never used.","React Hook useEffect has a missing dependency: 'startCaptureCanvas'. Either include it or remove the dependency array.",["242"],"React Hook useEffect has a missing dependency: 'drawLine'. Either include it or remove the dependency array.",["243"],"React Hook useEffect has a missing dependency: 'handleSuccess'. Either include it or remove the dependency array.",["244"],"strict","'use strict' is unnecessary inside of modules.","ExpressionStatement","module",{"range":"245","text":"246"},"'clipcount' is assigned a value but never used.",{"range":"247","text":"246"},["234"],["235"],"'Error' is assigned a value but never used.","no-global-assign","no-unsafe-negation",{"desc":"248","fix":"249"},{"desc":"250","fix":"251"},{"desc":"252","fix":"253"},{"desc":"250","fix":"254"},{"desc":"255","fix":"256"},{"desc":"248","fix":"257"},{"desc":"258","fix":"259"},{"desc":"260","fix":"261"},{"desc":"262","fix":"263"},[224,237],"",[224,237],"Update the dependencies array to be: [selectedAudioDevice, selectedAudioOutputDevice, selectedVideoDevice]",{"range":"264","text":"265"},"Update the dependencies array to be: [openCamera]",{"range":"266","text":"267"},"Update the dependencies array to be: [getMedia]",{"range":"268","text":"269"},{"range":"270","text":"267"},"Update the dependencies array to be: [openDevice]",{"range":"271","text":"272"},{"range":"273","text":"265"},"Update the dependencies array to be: [startCaptureCanvas]",{"range":"274","text":"275"},"Update the dependencies array to be: [drawLine]",{"range":"276","text":"277"},"Update the dependencies array to be: [handleSuccess]",{"range":"278","text":"279"},[2236,2238],"[selectedAudioDevice, selectedAudioOutputDevice, selectedVideoDevice]",[225,227],"[openCamera]",[1347,1349],"[getMedia]",[325,327],[271,273],"[openDevice]",[1361,1363],[524,526],"[startCaptureCanvas]",[341,343],"[drawLine]",[859,861],"[handleSuccess]"]